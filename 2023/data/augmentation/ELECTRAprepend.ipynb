{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlpgpu9/anaconda3/envs/eojin_conda/lib/python3.9/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, ElectraTokenizer, ElectraTokenizerFast, ElectraModel, DataCollatorForWholeWordMask, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import ElectraTokenizerFast, ElectraForMaskedLM, AdamW\n",
    "import random\n",
    "import logging\n",
    "import json\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "import torch.distributed as dist\n",
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import random\n",
    "import warnings\n",
    "from collections.abc import Mapping\n",
    "from dataclasses import dataclass\n",
    "from random import randint\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.utils import PaddingStrategy\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['기쁨', '기대', '믿음', '놀람', '혐오', '공포', '분노', '슬픔']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-generator\")\n",
    "model = ElectraForMaskedLM.from_pretrained(\"monologg/koelectra-base-v3-generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/nlpgpu9/ellt/eojin/EA/nikluge-ea-2023-train_수정_중복제거.jsonl\", 'r') as file:\n",
    "    data = []\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'nikluge-2023-ea-train-000001',\n",
       "  'input': {'form': '하... 근데 준프샤 너무 고소각임...',\n",
       "   'target': {'form': '준프샤', 'begin': 8, 'end': 11}},\n",
       "  'output': {'joy': 'True',\n",
       "   'anticipation': 'False',\n",
       "   'trust': 'False',\n",
       "   'surprise': 'False',\n",
       "   'disgust': 'False',\n",
       "   'fear': 'False',\n",
       "   'anger': 'False',\n",
       "   'sadness': 'False'}},\n",
       " {'id': 'nikluge-2023-ea-train-000002',\n",
       "  'input': {'form': '2기였나 지은북이랑 4기 메거진은 지금도 읽는데',\n",
       "   'target': {'form': '4기 메거진', 'begin': 11, 'end': 17}},\n",
       "  'output': {'joy': 'True',\n",
       "   'anticipation': 'False',\n",
       "   'trust': 'False',\n",
       "   'surprise': 'False',\n",
       "   'disgust': 'False',\n",
       "   'fear': 'False',\n",
       "   'anger': 'False',\n",
       "   'sadness': 'False'}},\n",
       " {'id': 'nikluge-2023-ea-train-000003',\n",
       "  'input': {'form': '흐아아아아악 흐아아아아악악악악 됭햄 손차이가 절케 난다니 알고는 있엇지만 놀랍다 ㅣ아아악악',\n",
       "   'target': {'form': '됭햄 손차이', 'begin': 17, 'end': 23}},\n",
       "  'output': {'joy': 'False',\n",
       "   'anticipation': 'False',\n",
       "   'trust': 'False',\n",
       "   'surprise': 'True',\n",
       "   'disgust': 'False',\n",
       "   'fear': 'False',\n",
       "   'anger': 'False',\n",
       "   'sadness': 'False'}},\n",
       " {'id': 'nikluge-2023-ea-train-000004',\n",
       "  'input': {'form': '도브가 반반을 가고 프린스가 안밀린다면 하는 상상',\n",
       "   'target': {'form': None, 'begin': None, 'end': None}},\n",
       "  'output': {'joy': 'False',\n",
       "   'anticipation': 'True',\n",
       "   'trust': 'False',\n",
       "   'surprise': 'False',\n",
       "   'disgust': 'False',\n",
       "   'fear': 'False',\n",
       "   'anger': 'False',\n",
       "   'sadness': 'False'}},\n",
       " {'id': 'nikluge-2023-ea-train-000005',\n",
       "  'input': {'form': '담주에 티켓팅 공지 뜨고 다담주에 티켓팅할 느낌',\n",
       "   'target': {'form': '티켓팅 공지', 'begin': 4, 'end': 10}},\n",
       "  'output': {'joy': 'False',\n",
       "   'anticipation': 'True',\n",
       "   'trust': 'False',\n",
       "   'surprise': 'False',\n",
       "   'disgust': 'False',\n",
       "   'fear': 'False',\n",
       "   'anger': 'False',\n",
       "   'sadness': 'False'}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/nlpgpu9/ellt/eojin/EA/nikluge-ea-2023-dev_수정.jsonl\", 'r') as file:\n",
    "    dev_data = []\n",
    "    for line in file:\n",
    "        dev_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputDataClass = NewType(\"InputDataClass\", Any)\n",
    "\n",
    "\"\"\"\n",
    "A DataCollator is a function that takes a list of samples from a Dataset and collate them into a batch, as a dictionary\n",
    "of PyTorch/TensorFlow tensors or NumPy arrays.\n",
    "\"\"\"\n",
    "DataCollator = NewType(\"DataCollator\", Callable[[List[InputDataClass]], Dict[str, Any]])\n",
    "\n",
    "\n",
    "class DataCollatorMixin:\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        if return_tensors is None:\n",
    "            return_tensors = self.return_tensors\n",
    "        if return_tensors == \"tf\":\n",
    "            return self.tf_call(features)\n",
    "        elif return_tensors == \"pt\":\n",
    "            return self.torch_call(features)\n",
    "        elif return_tensors == \"np\":\n",
    "            return self.numpy_call(features)\n",
    "        else:\n",
    "            raise ValueError(f\"Framework '{return_tensors}' not recognized!\")\n",
    "\n",
    "\n",
    "def default_data_collator(features: List[InputDataClass], return_tensors=\"pt\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Very simple data collator that simply collates batches of dict-like objects and performs special handling for\n",
    "    potential keys named:\n",
    "\n",
    "        - `label`: handles a single value (int or float) per object\n",
    "        - `label_ids`: handles a list of values per object\n",
    "\n",
    "    Does not do any additional preprocessing: property names of the input object will be used as corresponding inputs\n",
    "    to the model. See glue and ner for example of how it's useful.\n",
    "    \"\"\"\n",
    "\n",
    "    # In this function we'll make the assumption that all `features` in the batch\n",
    "    # have the same attributes.\n",
    "    # So we will look at the first element as a proxy for what attributes exist\n",
    "    # on the whole batch.\n",
    "\n",
    "    if return_tensors == \"pt\":\n",
    "        return torch_default_data_collator(features)\n",
    "    elif return_tensors == \"tf\":\n",
    "        return tf_default_data_collator(features)\n",
    "    elif return_tensors == \"np\":\n",
    "        return numpy_default_data_collator(features)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DefaultDataCollator(DataCollatorMixin):\n",
    "    \"\"\"\n",
    "    Very simple data collator that simply collates batches of dict-like objects and performs special handling for\n",
    "    potential keys named:\n",
    "\n",
    "        - `label`: handles a single value (int or float) per object\n",
    "        - `label_ids`: handles a list of values per object\n",
    "\n",
    "    Does not do any additional preprocessing: property names of the input object will be used as corresponding inputs\n",
    "    to the model. See glue and ner for example of how it's useful.\n",
    "\n",
    "    This is an object (like other data collators) rather than a pure function like default_data_collator. This can be\n",
    "    helpful if you need to set a return_tensors value at initialization.\n",
    "\n",
    "    Args:\n",
    "        return_tensors (`str`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]], return_tensors=None) -> Dict[str, Any]:\n",
    "        if return_tensors is None:\n",
    "            return_tensors = self.return_tensors\n",
    "        return default_data_collator(features, return_tensors)\n",
    "\n",
    "\n",
    "def torch_default_data_collator(features: List[InputDataClass]) -> Dict[str, Any]:\n",
    "    import torch\n",
    "\n",
    "    if not isinstance(features[0], Mapping):\n",
    "        features = [vars(f) for f in features]\n",
    "    first = features[0]\n",
    "    batch = {}\n",
    "\n",
    "    # Special handling for labels.\n",
    "    # Ensure that tensor is created with the correct type\n",
    "    # (it should be automatically the case, but let's make sure of it.)\n",
    "    if \"label\" in first and first[\"label\"] is not None:\n",
    "        label = first[\"label\"].item() if isinstance(first[\"label\"], torch.Tensor) else first[\"label\"]\n",
    "        dtype = torch.long if isinstance(label, int) else torch.float\n",
    "        batch[\"labels\"] = torch.tensor([f[\"label\"] for f in features], dtype=dtype)\n",
    "    elif \"label_ids\" in first and first[\"label_ids\"] is not None:\n",
    "        if isinstance(first[\"label_ids\"], torch.Tensor):\n",
    "            batch[\"labels\"] = torch.stack([f[\"label_ids\"] for f in features])\n",
    "        else:\n",
    "            dtype = torch.long if type(first[\"label_ids\"][0]) is int else torch.float\n",
    "            batch[\"labels\"] = torch.tensor([f[\"label_ids\"] for f in features], dtype=dtype)\n",
    "\n",
    "    # Handling of all other possible keys.\n",
    "    # Again, we will use the first element to figure out which key/values are not None for this model.\n",
    "    for k, v in first.items():\n",
    "        if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                batch[k] = torch.stack([f[k] for f in features])\n",
    "            elif isinstance(v, np.ndarray):\n",
    "                batch[k] = torch.tensor(np.stack([f[k] for f in features]))\n",
    "            else:\n",
    "                batch[k] = torch.tensor([f[k] for f in features])\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def tf_default_data_collator(features: List[InputDataClass]) -> Dict[str, Any]:\n",
    "    import tensorflow as tf\n",
    "\n",
    "    if not isinstance(features[0], Mapping):\n",
    "        features = [vars(f) for f in features]\n",
    "    first = features[0]\n",
    "    batch = {}\n",
    "\n",
    "    # Special handling for labels.\n",
    "    # Ensure that tensor is created with the correct type\n",
    "    # (it should be automatically the case, but let's make sure of it.)\n",
    "    if \"label\" in first and first[\"label\"] is not None:\n",
    "        label_col_name = \"label\"\n",
    "    elif \"label_ids\" in first and first[\"label_ids\"] is not None:\n",
    "        label_col_name = \"label_ids\"\n",
    "    elif \"labels\" in first and first[\"labels\"] is not None:\n",
    "        label_col_name = \"labels\"\n",
    "    else:\n",
    "        label_col_name = None\n",
    "    if label_col_name is not None:\n",
    "        if isinstance(first[label_col_name], tf.Tensor):\n",
    "            dtype = tf.int64 if first[label_col_name].dtype.is_integer else tf.float32\n",
    "        elif isinstance(first[label_col_name], np.ndarray) or isinstance(first[label_col_name], np.generic):\n",
    "            dtype = tf.int64 if np.issubdtype(first[label_col_name].dtype, np.integer) else tf.float32\n",
    "        elif isinstance(first[label_col_name], (tuple, list)):\n",
    "            dtype = tf.int64 if isinstance(first[label_col_name][0], int) else tf.float32\n",
    "        else:\n",
    "            dtype = tf.int64 if isinstance(first[label_col_name], int) else tf.float32\n",
    "        batch[\"labels\"] = tf.convert_to_tensor([f[label_col_name] for f in features], dtype=dtype)\n",
    "    # Handling of all other possible keys.\n",
    "    # Again, we will use the first element to figure out which key/values are not None for this model.\n",
    "    for k, v in first.items():\n",
    "        if k not in (\"label\", \"label_ids\", \"labels\") and v is not None and not isinstance(v, str):\n",
    "            if isinstance(v, (tf.Tensor, np.ndarray)):\n",
    "                batch[k] = tf.stack([f[k] for f in features])\n",
    "            else:\n",
    "                batch[k] = tf.convert_to_tensor([f[k] for f in features])\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def numpy_default_data_collator(features: List[InputDataClass]) -> Dict[str, Any]:\n",
    "    if not isinstance(features[0], Mapping):\n",
    "        features = [vars(f) for f in features]\n",
    "    first = features[0]\n",
    "    batch = {}\n",
    "\n",
    "    # Special handling for labels.\n",
    "    # Ensure that tensor is created with the correct type\n",
    "    # (it should be automatically the case, but let's make sure of it.)\n",
    "    if \"label\" in first and first[\"label\"] is not None:\n",
    "        label = first[\"label\"].item() if isinstance(first[\"label\"], np.ndarray) else first[\"label\"]\n",
    "        dtype = np.int64 if isinstance(label, int) else np.float32\n",
    "        batch[\"labels\"] = np.array([f[\"label\"] for f in features], dtype=dtype)\n",
    "    elif \"label_ids\" in first and first[\"label_ids\"] is not None:\n",
    "        if isinstance(first[\"label_ids\"], np.ndarray):\n",
    "            batch[\"labels\"] = np.stack([f[\"label_ids\"] for f in features])\n",
    "        else:\n",
    "            dtype = np.int64 if type(first[\"label_ids\"][0]) is int else np.float32\n",
    "            batch[\"labels\"] = np.array([f[\"label_ids\"] for f in features], dtype=dtype)\n",
    "\n",
    "    # Handling of all other possible keys.\n",
    "    # Again, we will use the first element to figure out which key/values are not None for this model.\n",
    "    for k, v in first.items():\n",
    "        if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n",
    "            if isinstance(v, np.ndarray):\n",
    "                batch[k] = np.stack([f[k] for f in features])\n",
    "            else:\n",
    "                batch[k] = np.array([f[k] for f in features])\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "\n",
    "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence is provided).\n",
    "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "              acceptable input length for the model if that argument is not provided.\n",
    "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
    "        max_length (`int`, *optional*):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        return_tensors (`str`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        if \"label\" in batch:\n",
    "            batch[\"labels\"] = batch[\"label\"]\n",
    "            del batch[\"label\"]\n",
    "        if \"label_ids\" in batch:\n",
    "            batch[\"labels\"] = batch[\"label_ids\"]\n",
    "            del batch[\"label_ids\"]\n",
    "        return batch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForTokenClassification(DataCollatorMixin):\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "\n",
    "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence is provided).\n",
    "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "              acceptable input length for the model if that argument is not provided.\n",
    "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
    "        max_length (`int`, *optional*):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        label_pad_token_id (`int`, *optional*, defaults to -100):\n",
    "            The id to use when padding the labels (-100 will be automatically ignore by PyTorch loss functions).\n",
    "        return_tensors (`str`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    label_pad_token_id: int = -100\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def torch_call(self, features):\n",
    "        import torch\n",
    "\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n",
    "\n",
    "        no_labels_features = [{k: v for k, v in feature.items() if k != label_name} for feature in features]\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            no_labels_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if labels is None:\n",
    "            return batch\n",
    "\n",
    "        sequence_length = batch[\"input_ids\"].shape[1]\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "\n",
    "        def to_list(tensor_or_iterable):\n",
    "            if isinstance(tensor_or_iterable, torch.Tensor):\n",
    "                return tensor_or_iterable.tolist()\n",
    "            return list(tensor_or_iterable)\n",
    "\n",
    "        if padding_side == \"right\":\n",
    "            batch[label_name] = [\n",
    "                to_list(label) + [self.label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "            ]\n",
    "        else:\n",
    "            batch[label_name] = [\n",
    "                [self.label_pad_token_id] * (sequence_length - len(label)) + to_list(label) for label in labels\n",
    "            ]\n",
    "\n",
    "        batch[label_name] = torch.tensor(batch[label_name], dtype=torch.int64)\n",
    "        return batch\n",
    "\n",
    "    def tf_call(self, features):\n",
    "        import tensorflow as tf\n",
    "\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n",
    "            return_tensors=\"tf\" if labels is None else None,\n",
    "        )\n",
    "\n",
    "        if labels is None:\n",
    "            return batch\n",
    "\n",
    "        sequence_length = tf.convert_to_tensor(batch[\"input_ids\"]).shape[1]\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "        if padding_side == \"right\":\n",
    "            batch[\"labels\"] = [\n",
    "                list(label) + [self.label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "            ]\n",
    "        else:\n",
    "            batch[\"labels\"] = [\n",
    "                [self.label_pad_token_id] * (sequence_length - len(label)) + list(label) for label in labels\n",
    "            ]\n",
    "\n",
    "        batch = {k: tf.convert_to_tensor(v, dtype=tf.int64) for k, v in batch.items()}\n",
    "        return batch\n",
    "\n",
    "    def numpy_call(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n",
    "            return_tensors=\"np\" if labels is None else None,\n",
    "        )\n",
    "\n",
    "        if labels is None:\n",
    "            return batch\n",
    "\n",
    "        sequence_length = np.array(batch[\"input_ids\"]).shape[1]\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "        if padding_side == \"right\":\n",
    "            batch[\"labels\"] = [\n",
    "                list(label) + [self.label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "            ]\n",
    "        else:\n",
    "            batch[\"labels\"] = [\n",
    "                [self.label_pad_token_id] * (sequence_length - len(label)) + list(label) for label in labels\n",
    "            ]\n",
    "\n",
    "        batch = {k: np.array(v, dtype=np.int64) for k, v in batch.items()}\n",
    "        return batch\n",
    "\n",
    "\n",
    "def _torch_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int] = None):\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "    import torch\n",
    "\n",
    "    # Tensorize if necessary.\n",
    "    if isinstance(examples[0], (list, tuple, np.ndarray)):\n",
    "        examples = [torch.tensor(e, dtype=torch.long) for e in examples]\n",
    "\n",
    "    length_of_first = examples[0].size(0)\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "\n",
    "    are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "        return torch.stack(examples, dim=0)\n",
    "\n",
    "    # If yes, check if we have a `pad_token`.\n",
    "    if tokenizer._pad_token is None:\n",
    "        raise ValueError(\n",
    "            \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "        )\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    max_length = max(x.size(0) for x in examples)\n",
    "    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "    result = examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)\n",
    "    for i, example in enumerate(examples):\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            result[i, : example.shape[0]] = example\n",
    "        else:\n",
    "            result[i, -example.shape[0] :] = example\n",
    "    return result\n",
    "\n",
    "\n",
    "def _tf_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int] = None):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "    # Tensorize if necessary.\n",
    "    if isinstance(examples[0], (list, tuple)):\n",
    "        examples = [tf.convert_to_tensor(e, dtype=tf.int64) for e in examples]\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "    length_of_first = len(examples[0])\n",
    "    are_tensors_same_length = all(len(x) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "        return tf.stack(examples, axis=0)\n",
    "\n",
    "    # If yes, check if we have a `pad_token`.\n",
    "    if tokenizer._pad_token is None:\n",
    "        raise ValueError(\n",
    "            \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "        )\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    max_length = max(len(x) for x in examples)\n",
    "    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "    # result = examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)\n",
    "    result = []\n",
    "    rank = tf.rank(examples[0])\n",
    "    paddings = np.zeros((rank, 2), dtype=np.int32)\n",
    "    for example in examples:\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            paddings[0, 1] = max_length - len(example)\n",
    "        else:\n",
    "            paddings[0, 0] = max_length - len(example)\n",
    "        result.append(tf.pad(example, paddings, constant_values=tokenizer.pad_token_id))\n",
    "    return tf.stack(result, axis=0)\n",
    "\n",
    "\n",
    "def _numpy_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int] = None):\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "    # Tensorize if necessary.\n",
    "    if isinstance(examples[0], (list, tuple)):\n",
    "        examples = [np.array(e, dtype=np.int64) for e in examples]\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "    length_of_first = len(examples[0])\n",
    "    are_tensors_same_length = all(len(x) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "        return np.stack(examples, axis=0)\n",
    "\n",
    "    # If yes, check if we have a `pad_token`.\n",
    "    if tokenizer._pad_token is None:\n",
    "        raise ValueError(\n",
    "            \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "        )\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    max_length = max(len(x) for x in examples)\n",
    "    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "    result = np.full(shape=(len(examples), max_length), fill_value=tokenizer.pad_token_id, dtype=examples[0].dtype)\n",
    "    for i, example in enumerate(examples):\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            result[i, : example.shape[0]] = example\n",
    "        else:\n",
    "            result[i, -example.shape[0] :] = example\n",
    "    return result\n",
    "\n",
    "\n",
    "def tolist(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    elif hasattr(x, \"numpy\"):  # Checks for TF tensors without needing the import\n",
    "        x = x.numpy()\n",
    "    return x.tolist()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSeq2Seq:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        model ([`PreTrainedModel`]):\n",
    "            The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to\n",
    "            prepare the *decoder_input_ids*\n",
    "\n",
    "            This is useful when using *label_smoothing* to avoid calculating loss twice.\n",
    "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "\n",
    "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence is provided).\n",
    "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "              acceptable input length for the model if that argument is not provided.\n",
    "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
    "        max_length (`int`, *optional*):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        label_pad_token_id (`int`, *optional*, defaults to -100):\n",
    "            The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).\n",
    "        return_tensors (`str`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    model: Optional[Any] = None\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    label_pad_token_id: int = -100\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        if return_tensors is None:\n",
    "            return_tensors = self.return_tensors\n",
    "        labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0].keys() else None\n",
    "        # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n",
    "        # same length to return tensors.\n",
    "        if labels is not None:\n",
    "            max_label_length = max(len(l) for l in labels)\n",
    "            if self.pad_to_multiple_of is not None:\n",
    "                max_label_length = (\n",
    "                    (max_label_length + self.pad_to_multiple_of - 1)\n",
    "                    // self.pad_to_multiple_of\n",
    "                    * self.pad_to_multiple_of\n",
    "                )\n",
    "\n",
    "            padding_side = self.tokenizer.padding_side\n",
    "            for feature in features:\n",
    "                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n",
    "                if isinstance(feature[\"labels\"], list):\n",
    "                    feature[\"labels\"] = (\n",
    "                        feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n",
    "                    )\n",
    "                elif padding_side == \"right\":\n",
    "                    feature[\"labels\"] = np.concatenate([feature[\"labels\"], remainder]).astype(np.int64)\n",
    "                else:\n",
    "                    feature[\"labels\"] = np.concatenate([remainder, feature[\"labels\"]]).astype(np.int64)\n",
    "\n",
    "        features = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=return_tensors,\n",
    "        )\n",
    "\n",
    "        # prepare decoder_input_ids\n",
    "        if (\n",
    "            labels is not None\n",
    "            and self.model is not None\n",
    "            and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n",
    "        ):\n",
    "            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features[\"labels\"])\n",
    "            features[\"decoder_input_ids\"] = decoder_input_ids\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorForLanguageModeling(DataCollatorMixin):\n",
    "    \"\"\"\n",
    "    Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they\n",
    "    are not all of the same length.\n",
    "\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        mlm (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to use masked language modeling. If set to `False`, the labels are the same as the inputs\n",
    "            with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for non-masked\n",
    "            tokens and the value to predict for the masked token.\n",
    "        mlm_probability (`float`, *optional*, defaults to 0.15):\n",
    "            The probability with which to (randomly) mask tokens in the input, when `mlm` is set to `True`.\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "        return_tensors (`str`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "\n",
    "    <Tip>\n",
    "\n",
    "    For best performance, this data collator should be used with a dataset having items that are dictionaries or\n",
    "    BatchEncoding, with the `\"special_tokens_mask\"` key, as returned by a [`PreTrainedTokenizer`] or a\n",
    "    [`PreTrainedTokenizerFast`] with the argument `return_special_tokens_mask=True`.\n",
    "\n",
    "    </Tip>\"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    mlm: bool = True\n",
    "    mlm_probability: float = 0.15\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    tf_experimental_compile: bool = False\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.mlm and self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. \"\n",
    "                \"You should pass `mlm=False` to train on causal language modeling instead.\"\n",
    "            )\n",
    "        if self.tf_experimental_compile:\n",
    "            import tensorflow as tf\n",
    "\n",
    "            self.tf_mask_tokens = tf.function(self.tf_mask_tokens, jit_compile=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def tf_bernoulli(shape, probability):\n",
    "        import tensorflow as tf\n",
    "\n",
    "        prob_matrix = tf.fill(shape, probability)\n",
    "        return tf.cast(prob_matrix - tf.random.uniform(shape, 0, 1) >= 0, tf.bool)\n",
    "\n",
    "    def tf_mask_tokens(\n",
    "        self, inputs: Any, vocab_size, mask_token_id, special_tokens_mask: Optional[Any] = None\n",
    "    ) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        import tensorflow as tf\n",
    "\n",
    "        mask_token_id = tf.cast(mask_token_id, inputs.dtype)\n",
    "\n",
    "        input_shape = tf.shape(inputs)\n",
    "        # 1 for a special token, 0 for a normal token in the special tokens mask\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        masked_indices = self.tf_bernoulli(input_shape, self.mlm_probability) & ~special_tokens_mask\n",
    "        # Replace unmasked indices with -100 in the labels since we only compute loss on masked tokens\n",
    "        labels = tf.where(masked_indices, inputs, -100)\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = self.tf_bernoulli(input_shape, 0.8) & masked_indices\n",
    "\n",
    "        inputs = tf.where(indices_replaced, mask_token_id, inputs)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = self.tf_bernoulli(input_shape, 0.1) & masked_indices & ~indices_replaced\n",
    "        random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=inputs.dtype)\n",
    "\n",
    "        inputs = tf.where(indices_random, random_words, inputs)\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "\n",
    "    def tf_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        import tensorflow as tf\n",
    "\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"tf\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\n",
    "                \"input_ids\": _tf_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "            }\n",
    "\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        if self.mlm:\n",
    "            if special_tokens_mask is None:\n",
    "                special_tokens_mask = [\n",
    "                    self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)\n",
    "                    for val in batch[\"input_ids\"].numpy().tolist()\n",
    "                ]\n",
    "                # Cannot directly create as bool\n",
    "                special_tokens_mask = tf.cast(tf.convert_to_tensor(special_tokens_mask, dtype=tf.int64), tf.bool)\n",
    "            else:\n",
    "                special_tokens_mask = tf.cast(special_tokens_mask, tf.bool)\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.tf_mask_tokens(\n",
    "                tf.cast(batch[\"input_ids\"], tf.int64),\n",
    "                special_tokens_mask=special_tokens_mask,\n",
    "                mask_token_id=self.tokenizer.mask_token_id,\n",
    "                vocab_size=len(self.tokenizer),\n",
    "            )\n",
    "        else:\n",
    "            labels = batch[\"input_ids\"]\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                # Replace self.tokenizer.pad_token_id with -100\n",
    "                labels = tf.where(labels == self.tokenizer.pad_token_id, -100, labels)\n",
    "            else:\n",
    "                labels = tf.identity(labels)  # Makes a copy, just in case\n",
    "            batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\n",
    "                \"input_ids\": _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "            }\n",
    "\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        if self.mlm:\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.torch_mask_tokens(\n",
    "                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "            )\n",
    "        else:\n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "\n",
    "    def numpy_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"np\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\n",
    "                \"input_ids\": _numpy_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "            }\n",
    "\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        if self.mlm:\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.numpy_mask_tokens(\n",
    "                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "            )\n",
    "        else:\n",
    "            labels = np.copy(batch[\"input_ids\"])\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    def numpy_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        labels = np.copy(inputs)\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = np.full(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = np.array(special_tokens_mask, dtype=bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.astype(bool)\n",
    "\n",
    "        probability_matrix[special_tokens_mask] = 0\n",
    "        # Numpy doesn't have bernoulli, so we use a binomial with 1 trial\n",
    "        masked_indices = np.random.binomial(1, probability_matrix, size=probability_matrix.shape).astype(bool)\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = np.random.binomial(1, 0.8, size=labels.shape).astype(bool) & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.mask_token_id\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        # indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        indices_random = (\n",
    "            np.random.binomial(1, 0.5, size=labels.shape).astype(bool) & masked_indices & ~indices_replaced\n",
    "        )\n",
    "        random_words = np.random.randint(\n",
    "            low=0, high=len(self.tokenizer), size=np.count_nonzero(indices_random), dtype=np.int64\n",
    "        )\n",
    "        inputs[indices_random] = random_words\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForWholeWordMask(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Data collator used for language modeling that masks entire words.\n",
    "\n",
    "    - collates batches of tensors, honoring their tokenizer's pad_token\n",
    "    - preprocesses batches for masked language modeling\n",
    "\n",
    "    <Tip>\n",
    "\n",
    "    This collator relies on details of the implementation of subword tokenization by [`BertTokenizer`], specifically\n",
    "    that subword tokens are prefixed with *##*. For tokenizers that do not adhere to this scheme, this collator will\n",
    "    produce an output that is roughly equivalent to [`.DataCollatorForLanguageModeling`].\n",
    "\n",
    "    </Tip>\"\"\"\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            input_ids = [e[\"input_ids\"] for e in examples]\n",
    "        else:\n",
    "            input_ids = examples\n",
    "            examples = [{\"input_ids\": e} for e in examples]\n",
    "\n",
    "        batch_input = _torch_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "\n",
    "        mask_labels = []\n",
    "        for e in examples:\n",
    "            ref_tokens = []\n",
    "            for id in tolist(e[\"input_ids\"]):\n",
    "                token = self.tokenizer._convert_id_to_token(id)\n",
    "                ref_tokens.append(token)\n",
    "\n",
    "            # For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-> [喜，##欢]\n",
    "            if \"chinese_ref\" in e:\n",
    "                ref_pos = tolist(e[\"chinese_ref\"])\n",
    "                len_seq = len(e[\"input_ids\"])\n",
    "                for i in range(len_seq):\n",
    "                    if i in ref_pos:\n",
    "                        ref_tokens[i] = \"##\" + ref_tokens[i]\n",
    "            mask_labels.append(self._whole_word_mask(ref_tokens))\n",
    "        batch_mask = _torch_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        inputs, labels = self.torch_mask_tokens(batch_input, batch_mask)\n",
    "        return {\"input_ids\": inputs, \"labels\": labels}\n",
    "\n",
    "    def tf_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        import tensorflow as tf\n",
    "\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            input_ids = [e[\"input_ids\"] for e in examples]\n",
    "        else:\n",
    "            input_ids = examples\n",
    "            examples = [{\"input_ids\": e} for e in examples]\n",
    "\n",
    "        batch_input = _tf_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "\n",
    "        mask_labels = []\n",
    "        for e in examples:\n",
    "            ref_tokens = []\n",
    "            for id in tolist(e[\"input_ids\"]):\n",
    "                token = self.tokenizer._convert_id_to_token(id)\n",
    "                ref_tokens.append(token)\n",
    "\n",
    "            # For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-> [喜，##欢]\n",
    "            if \"chinese_ref\" in e:\n",
    "                ref_pos = tolist(e[\"chinese_ref\"])\n",
    "                len_seq = len(e[\"input_ids\"])\n",
    "                for i in range(len_seq):\n",
    "                    if i in ref_pos:\n",
    "                        ref_tokens[i] = \"##\" + ref_tokens[i]\n",
    "            mask_labels.append(self._whole_word_mask(ref_tokens))\n",
    "        batch_mask = _tf_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        inputs, labels = self.tf_mask_tokens(tf.cast(batch_input, tf.int64), batch_mask)\n",
    "        return {\"input_ids\": inputs, \"labels\": labels}\n",
    "\n",
    "    def numpy_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            input_ids = [e[\"input_ids\"] for e in examples]\n",
    "        else:\n",
    "            input_ids = examples\n",
    "            examples = [{\"input_ids\": e} for e in examples]\n",
    "\n",
    "        batch_input = _numpy_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "\n",
    "        mask_labels = []\n",
    "        for e in examples:\n",
    "            ref_tokens = []\n",
    "            for id in tolist(e[\"input_ids\"]):\n",
    "                token = self.tokenizer._convert_id_to_token(id)\n",
    "                ref_tokens.append(token)\n",
    "\n",
    "            # For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-> [喜，##欢]\n",
    "            if \"chinese_ref\" in e:\n",
    "                ref_pos = tolist(e[\"chinese_ref\"])\n",
    "                len_seq = len(e[\"input_ids\"])\n",
    "                for i in range(len_seq):\n",
    "                    if i in ref_pos:\n",
    "                        ref_tokens[i] = \"##\" + ref_tokens[i]\n",
    "            mask_labels.append(self._whole_word_mask(ref_tokens))\n",
    "        batch_mask = _numpy_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        inputs, labels = self.numpy_mask_tokens(batch_input, batch_mask)\n",
    "        return {\"input_ids\": inputs, \"labels\": labels}\n",
    "\n",
    "    def _whole_word_mask(self, input_tokens: List[str], max_predictions=192):\n",
    "        \"\"\"\n",
    "        Get 0/1 labels for masked tokens with whole word mask proxy\n",
    "        \"\"\"\n",
    "        if not isinstance(self.tokenizer, (ElectraTokenizer, BertTokenizerFast)):\n",
    "            warnings.warn(\n",
    "                \"DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. \"\n",
    "                \"Please refer to the documentation for more information.\"\n",
    "            )\n",
    "\n",
    "        cand_indexes = []\n",
    "        for i, token in enumerate(input_tokens):\n",
    "            if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "                continue\n",
    "\n",
    "            if len(cand_indexes) >= 1 and token.startswith(\"##\"):\n",
    "                cand_indexes[-1].append(i)\n",
    "            else:\n",
    "                cand_indexes.append([i])\n",
    "\n",
    "        random.shuffle(cand_indexes)\n",
    "        num_to_predict = min(max_predictions, max(1, int(round(len(input_tokens) * self.mlm_probability))))\n",
    "        masked_lms = []\n",
    "        covered_indexes = set()\n",
    "        for index_set in cand_indexes:\n",
    "            if len(masked_lms) >= num_to_predict:\n",
    "                break\n",
    "            # If adding a whole-word mask would exceed the maximum number of\n",
    "            # predictions, then just skip this candidate.\n",
    "            if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "                continue\n",
    "            is_any_index_covered = False\n",
    "            for index in index_set:\n",
    "                if index in covered_indexes:\n",
    "                    is_any_index_covered = True\n",
    "                    break\n",
    "            if is_any_index_covered:\n",
    "                continue\n",
    "            for index in index_set:\n",
    "                covered_indexes.add(index)\n",
    "                masked_lms.append(index)\n",
    "\n",
    "        if len(covered_indexes) != len(masked_lms):\n",
    "            raise ValueError(\"Length of covered_indexes is not equal to length of masked_lms.\")\n",
    "        mask_labels = [1 if i in covered_indexes else 0 for i in range(len(input_tokens))]\n",
    "        return mask_labels\n",
    "\n",
    "    def torch_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n",
    "        'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the\"\n",
    "                \" --mlm flag if you want to use this tokenizer.\"\n",
    "            )\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "\n",
    "        probability_matrix = mask_labels\n",
    "\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "\n",
    "        masked_indices = probability_matrix.bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "\n",
    "    def tf_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n",
    "        'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n",
    "        \"\"\"\n",
    "        import tensorflow as tf\n",
    "\n",
    "        input_shape = tf.shape(inputs)\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the\"\n",
    "                \" --mlm flag if you want to use this tokenizer.\"\n",
    "            )\n",
    "        labels = tf.identity(inputs)\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "\n",
    "        masked_indices = tf.cast(mask_labels, tf.bool)\n",
    "\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels\n",
    "        ]\n",
    "        masked_indices = masked_indices & ~tf.cast(special_tokens_mask, dtype=tf.bool)\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = inputs == self.tokenizer.pad_token_id\n",
    "            masked_indices = masked_indices & ~padding_mask\n",
    "\n",
    "        # Replace unmasked indices with -100 in the labels since we only compute loss on masked tokens\n",
    "        labels = tf.where(masked_indices, inputs, -100)\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = self.tf_bernoulli(input_shape, 0.8) & masked_indices\n",
    "\n",
    "        inputs = tf.where(indices_replaced, self.tokenizer.mask_token_id, inputs)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = self.tf_bernoulli(input_shape, 0.5) & masked_indices & ~indices_replaced\n",
    "        random_words = tf.random.uniform(input_shape, maxval=len(self.tokenizer), dtype=tf.int64)\n",
    "        inputs = tf.where(indices_random, random_words, inputs)\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "\n",
    "    def numpy_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n",
    "        'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n",
    "        \"\"\"\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the\"\n",
    "                \" --mlm flag if you want to use this tokenizer.\"\n",
    "            )\n",
    "        labels = np.copy(inputs)\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "\n",
    "        masked_indices = mask_labels.astype(bool)\n",
    "\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        masked_indices[np.array(special_tokens_mask, dtype=bool)] = 0\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels == self.tokenizer.pad_token_id\n",
    "            masked_indices[padding_mask] = 0\n",
    "\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = np.random.binomial(1, 0.8, size=labels.shape).astype(bool) & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        # indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        indices_random = (\n",
    "            np.random.binomial(1, 0.5, size=labels.shape).astype(bool) & masked_indices & ~indices_replaced\n",
    "        )\n",
    "        random_words = np.random.randint(low=0, high=len(self.tokenizer), size=labels.shape, dtype=np.int64)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_labels_to_text(preprocessed_data, label_mapping):\n",
    "    prepended_data = []\n",
    "    \n",
    "    for data_point in preprocessed_data:\n",
    "        labels = data_point['output']\n",
    "        masked_text = data_point['input']['form']\n",
    "        target_begin = data_point['input']['target']['begin']\n",
    "        target_end = data_point['input']['target']['end']\n",
    "        target = data_point['input']['target']['form']\n",
    "\n",
    "        true_count = sum(1 for value in labels.values() if value == 'True')\n",
    "\n",
    "        if target != None:\n",
    "            if true_count == 1:\n",
    "                target_begin += 3\n",
    "                target_end += 3\n",
    "            elif true_count == 2:\n",
    "                target_begin += 6\n",
    "                target_end += 6\n",
    "            else:\n",
    "                target_begin += 9\n",
    "                target_end += 9\n",
    "        \n",
    "        # 라벨 찾기\n",
    "        prepend_str = ' '.join([f'{label_mapping[label]}' for label, value in labels.items() if value == 'True'])\n",
    "        \n",
    "        new_text = f\"{prepend_str} {masked_text}\" if prepend_str else masked_text\n",
    "\n",
    "    \n",
    "\n",
    "        # 데이터셋 만들기\n",
    "        prepended_data.append({\n",
    "            'id': data_point['id'],\n",
    "            'prepended_text': new_text,\n",
    "            'target': {\n",
    "                    'form': data_point['input']['target']['form'],\n",
    "                    'begin': target_begin,\n",
    "                    'end': target_end\n",
    "}\n",
    "        })\n",
    "        \n",
    "    return prepended_data\n",
    "\n",
    "# 라벨은 한국어로\n",
    "label_mapping = {\n",
    "    'joy': '기쁨',\n",
    "    'anticipation': '기대',\n",
    "    'trust': '믿음',\n",
    "    'surprise': '놀람',\n",
    "    'disgust': '혐오',\n",
    "    'fear': '공포',\n",
    "    'anger': '분노',\n",
    "    'sadness': '슬픔'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'nikluge-2023-ea-train-000001', 'prepended_text': '기쁨 하... 근데 준프샤 너무 고소각임...', 'target': {'form': '준프샤', 'begin': 11, 'end': 14}}, {'id': 'nikluge-2023-ea-train-000002', 'prepended_text': '기쁨 2기였나 지은북이랑 4기 메거진은 지금도 읽는데', 'target': {'form': '4기 메거진', 'begin': 14, 'end': 20}}, {'id': 'nikluge-2023-ea-train-000003', 'prepended_text': '놀람 흐아아아아악 흐아아아아악악악악 됭햄 손차이가 절케 난다니 알고는 있엇지만 놀랍다 ㅣ아아악악', 'target': {'form': '됭햄 손차이', 'begin': 20, 'end': 26}}, {'id': 'nikluge-2023-ea-train-000004', 'prepended_text': '기대 도브가 반반을 가고 프린스가 안밀린다면 하는 상상', 'target': {'form': None, 'begin': None, 'end': None}}, {'id': 'nikluge-2023-ea-train-000005', 'prepended_text': '기대 담주에 티켓팅 공지 뜨고 다담주에 티켓팅할 느낌', 'target': {'form': '티켓팅 공지', 'begin': 7, 'end': 13}}]\n"
     ]
    }
   ],
   "source": [
    "prepended_data = prepend_labels_to_text(data, label_mapping)\n",
    "print(prepended_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'nikluge-2023-ea-dev-000001', 'prepended_text': '기대 하,,,,내일 옥상다이브 하기 전에 표 구하길 기도해주세요', 'target': {'form': '표', 'begin': 23, 'end': 24}}, {'id': 'nikluge-2023-ea-dev-000002', 'prepended_text': '기쁨 밴드 사운드 진짜 너무너무 좋다.... 꿈인가 진짜', 'target': {'form': '밴드 사운드', 'begin': 3, 'end': 9}}, {'id': 'nikluge-2023-ea-dev-000003', 'prepended_text': '기쁨 칸 태리 너무 풋풋하고 귀여워.. 근데 허리가 한줌이셔 &others&', 'target': {'form': '칸 태리', 'begin': 3, 'end': 7}}, {'id': 'nikluge-2023-ea-dev-000004', 'prepended_text': '기대 미스터초밥왕이 햄스터 해바라기씨가 되어 오는 기분보다는 더 낫지 싶음..', 'target': {'form': '기분', 'begin': 28, 'end': 30}}, {'id': 'nikluge-2023-ea-dev-000005', 'prepended_text': '놀람 테렞 초연 자첫날 진심 한 마디 안에 멜로디 무시하고 가사 너무 많이 들어간 것 같아서 ??? 했는데 그 뒤로 그런극 많이 본 것 같아서 이제 놀랍진 않음', 'target': {'form': '테렞 초연 자첫날', 'begin': 3, 'end': 12}}]\n"
     ]
    }
   ],
   "source": [
    "dev_prepended_data = prepend_labels_to_text(dev_data, label_mapping)\n",
    "print(dev_prepended_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_prepended_text(data_entry):\n",
    "#     text = data_entry['prepended_text'].split()\n",
    "#     emotions = [word for word in text if word in emotions_labels]\n",
    "#     remaining_text = \" \".join([word for word in text if word not in emotions])\n",
    "#     new_prepended_text = f\"[CLS] {' '.join(emotions)} [SEP] {remaining_text} [SEP]\"\n",
    "#     return new_prepended_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Update 'prepended_text' for each entry in the data\n",
    "# for entry in prepended_data:\n",
    "#     entry['prepended_text'] = update_prepended_text(entry)\n",
    "\n",
    "# prepended_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Update 'prepended_text' for each entry in the data\n",
    "# for entry in dev_prepended_data:\n",
    "#     entry['prepended_text'] = update_prepended_text(entry)\n",
    "\n",
    "# dev_prepended_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputDataClass = NewType(\"InputDataClass\", Any)\n",
    "\n",
    "\"\"\"\n",
    "A DataCollator is a function that takes a list of samples from a Dataset and collate them into a batch, as a dictionary\n",
    "of PyTorch/TensorFlow tensors or NumPy arrays.\n",
    "\"\"\"\n",
    "DataCollator = NewType(\"DataCollator\", Callable[[List[InputDataClass]], Dict[str, Any]])\n",
    "\n",
    "\n",
    "class DataCollatorMixin:\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        if return_tensors is None:\n",
    "            return_tensors = self.return_tensors\n",
    "        if return_tensors == \"tf\":\n",
    "            return self.tf_call(features)\n",
    "        elif return_tensors == \"pt\":\n",
    "            return self.torch_call(features)\n",
    "        elif return_tensors == \"np\":\n",
    "            return self.numpy_call(features)\n",
    "        else:\n",
    "            raise ValueError(f\"Framework '{return_tensors}' not recognized!\")\n",
    "\n",
    "\n",
    "def default_data_collator(features: List[InputDataClass], return_tensors=\"pt\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Very simple data collator that simply collates batches of dict-like objects and performs special handling for\n",
    "    potential keys named:\n",
    "\n",
    "        - `label`: handles a single value (int or float) per object\n",
    "        - `label_ids`: handles a list of values per object\n",
    "\n",
    "    Does not do any additional preprocessing: property names of the input object will be used as corresponding inputs\n",
    "    to the model. See glue and ner for example of how it's useful.\n",
    "    \"\"\"\n",
    "\n",
    "    # In this function we'll make the assumption that all `features` in the batch\n",
    "    # have the same attributes.\n",
    "    # So we will look at the first element as a proxy for what attributes exist\n",
    "    # on the whole batch.\n",
    "\n",
    "    if return_tensors == \"pt\":\n",
    "        return torch_default_data_collator(features)\n",
    "    elif return_tensors == \"tf\":\n",
    "        return tf_default_data_collator(features)\n",
    "    elif return_tensors == \"np\":\n",
    "        return numpy_default_data_collator(features)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DefaultDataCollator(DataCollatorMixin):\n",
    "    \"\"\"\n",
    "    Very simple data collator that simply collates batches of dict-like objects and performs special handling for\n",
    "    potential keys named:\n",
    "\n",
    "        - `label`: handles a single value (int or float) per object\n",
    "        - `label_ids`: handles a list of values per object\n",
    "\n",
    "    Does not do any additional preprocessing: property names of the input object will be used as corresponding inputs\n",
    "    to the model. See glue and ner for example of how it's useful.\n",
    "\n",
    "    This is an object (like other data collators) rather than a pure function like default_data_collator. This can be\n",
    "    helpful if you need to set a return_tensors value at initialization.\n",
    "\n",
    "    Args:\n",
    "        return_tensors (`str`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]], return_tensors=None) -> Dict[str, Any]:\n",
    "        if return_tensors is None:\n",
    "            return_tensors = self.return_tensors\n",
    "        return default_data_collator(features, return_tensors)\n",
    "\n",
    "\n",
    "def torch_default_data_collator(features: List[InputDataClass]) -> Dict[str, Any]:\n",
    "    import torch\n",
    "\n",
    "    if not isinstance(features[0], Mapping):\n",
    "        features = [vars(f) for f in features]\n",
    "    first = features[0]\n",
    "    batch = {}\n",
    "\n",
    "    # Special handling for labels.\n",
    "    # Ensure that tensor is created with the correct type\n",
    "    # (it should be automatically the case, but let's make sure of it.)\n",
    "    if \"label\" in first and first[\"label\"] is not None:\n",
    "        label = first[\"label\"].item() if isinstance(first[\"label\"], torch.Tensor) else first[\"label\"]\n",
    "        dtype = torch.long if isinstance(label, int) else torch.float\n",
    "        batch[\"labels\"] = torch.tensor([f[\"label\"] for f in features], dtype=dtype)\n",
    "    elif \"label_ids\" in first and first[\"label_ids\"] is not None:\n",
    "        if isinstance(first[\"label_ids\"], torch.Tensor):\n",
    "            batch[\"labels\"] = torch.stack([f[\"label_ids\"] for f in features])\n",
    "        else:\n",
    "            dtype = torch.long if type(first[\"label_ids\"][0]) is int else torch.float\n",
    "            batch[\"labels\"] = torch.tensor([f[\"label_ids\"] for f in features], dtype=dtype)\n",
    "\n",
    "    # Handling of all other possible keys.\n",
    "    # Again, we will use the first element to figure out which key/values are not None for this model.\n",
    "    for k, v in first.items():\n",
    "        if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                batch[k] = torch.stack([f[k] for f in features])\n",
    "            elif isinstance(v, np.ndarray):\n",
    "                batch[k] = torch.tensor(np.stack([f[k] for f in features]))\n",
    "            else:\n",
    "                batch[k] = torch.tensor([f[k] for f in features])\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def tf_default_data_collator(features: List[InputDataClass]) -> Dict[str, Any]:\n",
    "    import tensorflow as tf\n",
    "\n",
    "    if not isinstance(features[0], Mapping):\n",
    "        features = [vars(f) for f in features]\n",
    "    first = features[0]\n",
    "    batch = {}\n",
    "\n",
    "    # Special handling for labels.\n",
    "    # Ensure that tensor is created with the correct type\n",
    "    # (it should be automatically the case, but let's make sure of it.)\n",
    "    if \"label\" in first and first[\"label\"] is not None:\n",
    "        label_col_name = \"label\"\n",
    "    elif \"label_ids\" in first and first[\"label_ids\"] is not None:\n",
    "        label_col_name = \"label_ids\"\n",
    "    elif \"labels\" in first and first[\"labels\"] is not None:\n",
    "        label_col_name = \"labels\"\n",
    "    else:\n",
    "        label_col_name = None\n",
    "    if label_col_name is not None:\n",
    "        if isinstance(first[label_col_name], tf.Tensor):\n",
    "            dtype = tf.int64 if first[label_col_name].dtype.is_integer else tf.float32\n",
    "        elif isinstance(first[label_col_name], np.ndarray) or isinstance(first[label_col_name], np.generic):\n",
    "            dtype = tf.int64 if np.issubdtype(first[label_col_name].dtype, np.integer) else tf.float32\n",
    "        elif isinstance(first[label_col_name], (tuple, list)):\n",
    "            dtype = tf.int64 if isinstance(first[label_col_name][0], int) else tf.float32\n",
    "        else:\n",
    "            dtype = tf.int64 if isinstance(first[label_col_name], int) else tf.float32\n",
    "        batch[\"labels\"] = tf.convert_to_tensor([f[label_col_name] for f in features], dtype=dtype)\n",
    "    # Handling of all other possible keys.\n",
    "    # Again, we will use the first element to figure out which key/values are not None for this model.\n",
    "    for k, v in first.items():\n",
    "        if k not in (\"label\", \"label_ids\", \"labels\") and v is not None and not isinstance(v, str):\n",
    "            if isinstance(v, (tf.Tensor, np.ndarray)):\n",
    "                batch[k] = tf.stack([f[k] for f in features])\n",
    "            else:\n",
    "                batch[k] = tf.convert_to_tensor([f[k] for f in features])\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def numpy_default_data_collator(features: List[InputDataClass]) -> Dict[str, Any]:\n",
    "    if not isinstance(features[0], Mapping):\n",
    "        features = [vars(f) for f in features]\n",
    "    first = features[0]\n",
    "    batch = {}\n",
    "\n",
    "    # Special handling for labels.\n",
    "    # Ensure that tensor is created with the correct type\n",
    "    # (it should be automatically the case, but let's make sure of it.)\n",
    "    if \"label\" in first and first[\"label\"] is not None:\n",
    "        label = first[\"label\"].item() if isinstance(first[\"label\"], np.ndarray) else first[\"label\"]\n",
    "        dtype = np.int64 if isinstance(label, int) else np.float32\n",
    "        batch[\"labels\"] = np.array([f[\"label\"] for f in features], dtype=dtype)\n",
    "    elif \"label_ids\" in first and first[\"label_ids\"] is not None:\n",
    "        if isinstance(first[\"label_ids\"], np.ndarray):\n",
    "            batch[\"labels\"] = np.stack([f[\"label_ids\"] for f in features])\n",
    "        else:\n",
    "            dtype = np.int64 if type(first[\"label_ids\"][0]) is int else np.float32\n",
    "            batch[\"labels\"] = np.array([f[\"label_ids\"] for f in features], dtype=dtype)\n",
    "\n",
    "    # Handling of all other possible keys.\n",
    "    # Again, we will use the first element to figure out which key/values are not None for this model.\n",
    "    for k, v in first.items():\n",
    "        if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n",
    "            if isinstance(v, np.ndarray):\n",
    "                batch[k] = np.stack([f[k] for f in features])\n",
    "            else:\n",
    "                batch[k] = np.array([f[k] for f in features])\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "\n",
    "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence is provided).\n",
    "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "              acceptable input length for the model if that argument is not provided.\n",
    "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
    "        max_length (`int`, *optional*):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        return_tensors (`str`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        if \"label\" in batch:\n",
    "            batch[\"labels\"] = batch[\"label\"]\n",
    "            del batch[\"label\"]\n",
    "        if \"label_ids\" in batch:\n",
    "            batch[\"labels\"] = batch[\"label_ids\"]\n",
    "            del batch[\"label_ids\"]\n",
    "        return batch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForTokenClassification(DataCollatorMixin):\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "\n",
    "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence is provided).\n",
    "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "              acceptable input length for the model if that argument is not provided.\n",
    "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
    "        max_length (`int`, *optional*):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        label_pad_token_id (`int`, *optional*, defaults to -100):\n",
    "            The id to use when padding the labels (-100 will be automatically ignore by PyTorch loss functions).\n",
    "        return_tensors (`str`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    label_pad_token_id: int = -100\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def torch_call(self, features):\n",
    "        import torch\n",
    "\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n",
    "\n",
    "        no_labels_features = [{k: v for k, v in feature.items() if k != label_name} for feature in features]\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            no_labels_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if labels is None:\n",
    "            return batch\n",
    "\n",
    "        sequence_length = batch[\"input_ids\"].shape[1]\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "\n",
    "        def to_list(tensor_or_iterable):\n",
    "            if isinstance(tensor_or_iterable, torch.Tensor):\n",
    "                return tensor_or_iterable.tolist()\n",
    "            return list(tensor_or_iterable)\n",
    "\n",
    "        if padding_side == \"right\":\n",
    "            batch[label_name] = [\n",
    "                to_list(label) + [self.label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "            ]\n",
    "        else:\n",
    "            batch[label_name] = [\n",
    "                [self.label_pad_token_id] * (sequence_length - len(label)) + to_list(label) for label in labels\n",
    "            ]\n",
    "\n",
    "        batch[label_name] = torch.tensor(batch[label_name], dtype=torch.int64)\n",
    "        return batch\n",
    "\n",
    "    def tf_call(self, features):\n",
    "        import tensorflow as tf\n",
    "\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n",
    "            return_tensors=\"tf\" if labels is None else None,\n",
    "        )\n",
    "\n",
    "        if labels is None:\n",
    "            return batch\n",
    "\n",
    "        sequence_length = tf.convert_to_tensor(batch[\"input_ids\"]).shape[1]\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "        if padding_side == \"right\":\n",
    "            batch[\"labels\"] = [\n",
    "                list(label) + [self.label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "            ]\n",
    "        else:\n",
    "            batch[\"labels\"] = [\n",
    "                [self.label_pad_token_id] * (sequence_length - len(label)) + list(label) for label in labels\n",
    "            ]\n",
    "\n",
    "        batch = {k: tf.convert_to_tensor(v, dtype=tf.int64) for k, v in batch.items()}\n",
    "        return batch\n",
    "\n",
    "    def numpy_call(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n",
    "            return_tensors=\"np\" if labels is None else None,\n",
    "        )\n",
    "\n",
    "        if labels is None:\n",
    "            return batch\n",
    "\n",
    "        sequence_length = np.array(batch[\"input_ids\"]).shape[1]\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "        if padding_side == \"right\":\n",
    "            batch[\"labels\"] = [\n",
    "                list(label) + [self.label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "            ]\n",
    "        else:\n",
    "            batch[\"labels\"] = [\n",
    "                [self.label_pad_token_id] * (sequence_length - len(label)) + list(label) for label in labels\n",
    "            ]\n",
    "\n",
    "        batch = {k: np.array(v, dtype=np.int64) for k, v in batch.items()}\n",
    "        return batch\n",
    "\n",
    "\n",
    "def _torch_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int] = None):\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "    import torch\n",
    "\n",
    "    # Tensorize if necessary.\n",
    "    if isinstance(examples[0], (list, tuple, np.ndarray)):\n",
    "        examples = [torch.tensor(e, dtype=torch.long) for e in examples]\n",
    "\n",
    "    length_of_first = examples[0].size(0)\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "\n",
    "    are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "        return torch.stack(examples, dim=0)\n",
    "\n",
    "    # If yes, check if we have a `pad_token`.\n",
    "    if tokenizer._pad_token is None:\n",
    "        raise ValueError(\n",
    "            \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "        )\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    max_length = max(x.size(0) for x in examples)\n",
    "    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "    result = examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)\n",
    "    for i, example in enumerate(examples):\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            result[i, : example.shape[0]] = example\n",
    "        else:\n",
    "            result[i, -example.shape[0] :] = example\n",
    "    return result\n",
    "\n",
    "\n",
    "def _tf_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int] = None):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "    # Tensorize if necessary.\n",
    "    if isinstance(examples[0], (list, tuple)):\n",
    "        examples = [tf.convert_to_tensor(e, dtype=tf.int64) for e in examples]\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "    length_of_first = len(examples[0])\n",
    "    are_tensors_same_length = all(len(x) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "        return tf.stack(examples, axis=0)\n",
    "\n",
    "    # If yes, check if we have a `pad_token`.\n",
    "    if tokenizer._pad_token is None:\n",
    "        raise ValueError(\n",
    "            \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "        )\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    max_length = max(len(x) for x in examples)\n",
    "    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "    # result = examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)\n",
    "    result = []\n",
    "    rank = tf.rank(examples[0])\n",
    "    paddings = np.zeros((rank, 2), dtype=np.int32)\n",
    "    for example in examples:\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            paddings[0, 1] = max_length - len(example)\n",
    "        else:\n",
    "            paddings[0, 0] = max_length - len(example)\n",
    "        result.append(tf.pad(example, paddings, constant_values=tokenizer.pad_token_id))\n",
    "    return tf.stack(result, axis=0)\n",
    "\n",
    "\n",
    "def _numpy_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int] = None):\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "    # Tensorize if necessary.\n",
    "    if isinstance(examples[0], (list, tuple)):\n",
    "        examples = [np.array(e, dtype=np.int64) for e in examples]\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "    length_of_first = len(examples[0])\n",
    "    are_tensors_same_length = all(len(x) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "        return np.stack(examples, axis=0)\n",
    "\n",
    "    # If yes, check if we have a `pad_token`.\n",
    "    if tokenizer._pad_token is None:\n",
    "        raise ValueError(\n",
    "            \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "        )\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    max_length = max(len(x) for x in examples)\n",
    "    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "    result = np.full(shape=(len(examples), max_length), fill_value=tokenizer.pad_token_id, dtype=examples[0].dtype)\n",
    "    for i, example in enumerate(examples):\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            result[i, : example.shape[0]] = example\n",
    "        else:\n",
    "            result[i, -example.shape[0] :] = example\n",
    "    return result\n",
    "\n",
    "\n",
    "def tolist(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    elif hasattr(x, \"numpy\"):  # Checks for TF tensors without needing the import\n",
    "        x = x.numpy()\n",
    "    return x.tolist()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSeq2Seq:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        model ([`PreTrainedModel`]):\n",
    "            The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to\n",
    "            prepare the *decoder_input_ids*\n",
    "\n",
    "            This is useful when using *label_smoothing* to avoid calculating loss twice.\n",
    "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "\n",
    "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence is provided).\n",
    "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "              acceptable input length for the model if that argument is not provided.\n",
    "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
    "        max_length (`int`, *optional*):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        label_pad_token_id (`int`, *optional*, defaults to -100):\n",
    "            The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).\n",
    "        return_tensors (`str`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    model: Optional[Any] = None\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    label_pad_token_id: int = -100\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        if return_tensors is None:\n",
    "            return_tensors = self.return_tensors\n",
    "        labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0].keys() else None\n",
    "        # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n",
    "        # same length to return tensors.\n",
    "        if labels is not None:\n",
    "            max_label_length = max(len(l) for l in labels)\n",
    "            if self.pad_to_multiple_of is not None:\n",
    "                max_label_length = (\n",
    "                    (max_label_length + self.pad_to_multiple_of - 1)\n",
    "                    // self.pad_to_multiple_of\n",
    "                    * self.pad_to_multiple_of\n",
    "                )\n",
    "\n",
    "            padding_side = self.tokenizer.padding_side\n",
    "            for feature in features:\n",
    "                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n",
    "                if isinstance(feature[\"labels\"], list):\n",
    "                    feature[\"labels\"] = (\n",
    "                        feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n",
    "                    )\n",
    "                elif padding_side == \"right\":\n",
    "                    feature[\"labels\"] = np.concatenate([feature[\"labels\"], remainder]).astype(np.int64)\n",
    "                else:\n",
    "                    feature[\"labels\"] = np.concatenate([remainder, feature[\"labels\"]]).astype(np.int64)\n",
    "\n",
    "        features = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=return_tensors,\n",
    "        )\n",
    "\n",
    "        # prepare decoder_input_ids\n",
    "        if (\n",
    "            labels is not None\n",
    "            and self.model is not None\n",
    "            and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n",
    "        ):\n",
    "            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features[\"labels\"])\n",
    "            features[\"decoder_input_ids\"] = decoder_input_ids\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForLanguageModeling(DataCollatorMixin):\n",
    "    \"\"\"\n",
    "    Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they\n",
    "    are not all of the same length.\n",
    "\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        mlm (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to use masked language modeling. If set to `False`, the labels are the same as the inputs\n",
    "            with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for non-masked\n",
    "            tokens and the value to predict for the masked token.\n",
    "        mlm_probability (`float`, *optional*, defaults to 0.15):\n",
    "            The probability with which to (randomly) mask tokens in the input, when `mlm` is set to `True`.\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "        return_tensors (`str`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "\n",
    "    <Tip>\n",
    "\n",
    "    For best performance, this data collator should be used with a dataset having items that are dictionaries or\n",
    "    BatchEncoding, with the `\"special_tokens_mask\"` key, as returned by a [`PreTrainedTokenizer`] or a\n",
    "    [`PreTrainedTokenizerFast`] with the argument `return_special_tokens_mask=True`.\n",
    "\n",
    "    </Tip>\"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    mlm: bool = True\n",
    "    mlm_probability: float = 0.15\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    tf_experimental_compile: bool = False\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.mlm and self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. \"\n",
    "                \"You should pass `mlm=False` to train on causal language modeling instead.\"\n",
    "            )\n",
    "        if self.tf_experimental_compile:\n",
    "            import tensorflow as tf\n",
    "\n",
    "            self.tf_mask_tokens = tf.function(self.tf_mask_tokens, jit_compile=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def tf_bernoulli(shape, probability):\n",
    "        import tensorflow as tf\n",
    "\n",
    "        prob_matrix = tf.fill(shape, probability)\n",
    "        return tf.cast(prob_matrix - tf.random.uniform(shape, 0, 1) >= 0, tf.bool)\n",
    "\n",
    "    def tf_mask_tokens(\n",
    "        self, inputs: Any, vocab_size, mask_token_id, special_tokens_mask: Optional[Any] = None\n",
    "    ) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        import tensorflow as tf\n",
    "\n",
    "        mask_token_id = tf.cast(mask_token_id, inputs.dtype)\n",
    "\n",
    "        input_shape = tf.shape(inputs)\n",
    "        # 1 for a special token, 0 for a normal token in the special tokens mask\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        masked_indices = self.tf_bernoulli(input_shape, self.mlm_probability) & ~special_tokens_mask\n",
    "        # Replace unmasked indices with -100 in the labels since we only compute loss on masked tokens\n",
    "        labels = tf.where(masked_indices, inputs, -100)\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = self.tf_bernoulli(input_shape, 0.8) & masked_indices\n",
    "\n",
    "        inputs = tf.where(indices_replaced, mask_token_id, inputs)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = self.tf_bernoulli(input_shape, 0.1) & masked_indices & ~indices_replaced\n",
    "        random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=inputs.dtype)\n",
    "\n",
    "        inputs = tf.where(indices_random, random_words, inputs)\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "\n",
    "    def tf_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        import tensorflow as tf\n",
    "\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"tf\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\n",
    "                \"input_ids\": _tf_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "            }\n",
    "\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        if self.mlm:\n",
    "            if special_tokens_mask is None:\n",
    "                special_tokens_mask = [\n",
    "                    self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)\n",
    "                    for val in batch[\"input_ids\"].numpy().tolist()\n",
    "                ]\n",
    "                # Cannot directly create as bool\n",
    "                special_tokens_mask = tf.cast(tf.convert_to_tensor(special_tokens_mask, dtype=tf.int64), tf.bool)\n",
    "            else:\n",
    "                special_tokens_mask = tf.cast(special_tokens_mask, tf.bool)\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.tf_mask_tokens(\n",
    "                tf.cast(batch[\"input_ids\"], tf.int64),\n",
    "                special_tokens_mask=special_tokens_mask,\n",
    "                mask_token_id=self.tokenizer.mask_token_id,\n",
    "                vocab_size=len(self.tokenizer),\n",
    "            )\n",
    "        else:\n",
    "            labels = batch[\"input_ids\"]\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                # Replace self.tokenizer.pad_token_id with -100\n",
    "                labels = tf.where(labels == self.tokenizer.pad_token_id, -100, labels)\n",
    "            else:\n",
    "                labels = tf.identity(labels)  # Makes a copy, just in case\n",
    "            batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\n",
    "                \"input_ids\": _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "            }\n",
    "\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        if self.mlm:\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.torch_mask_tokens(\n",
    "                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "            )\n",
    "        else:\n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "\n",
    "    def numpy_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"np\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\n",
    "                \"input_ids\": _numpy_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "            }\n",
    "\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        if self.mlm:\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.numpy_mask_tokens(\n",
    "                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "            )\n",
    "        else:\n",
    "            labels = np.copy(batch[\"input_ids\"])\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    def numpy_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        labels = np.copy(inputs)\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = np.full(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = np.array(special_tokens_mask, dtype=bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.astype(bool)\n",
    "\n",
    "        probability_matrix[special_tokens_mask] = 0\n",
    "        # Numpy doesn't have bernoulli, so we use a binomial with 1 trial\n",
    "        masked_indices = np.random.binomial(1, probability_matrix, size=probability_matrix.shape).astype(bool)\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = np.random.binomial(1, 0.8, size=labels.shape).astype(bool) & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.mask_token_id\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        # indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        indices_random = (\n",
    "            np.random.binomial(1, 0.5, size=labels.shape).astype(bool) & masked_indices & ~indices_replaced\n",
    "        )\n",
    "        random_words = np.random.randint(\n",
    "            low=0, high=len(self.tokenizer), size=np.count_nonzero(indices_random), dtype=np.int64\n",
    "        )\n",
    "        inputs[indices_random] = random_words\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForWholeWordMask(DataCollatorForLanguageModeling):\n",
    "    \n",
    "    \"\"\"\n",
    "    Data collator used for language modeling that masks entire words.\n",
    "\n",
    "    - collates batches of tensors, honoring their tokenizer's pad_token\n",
    "    - preprocesses batches for masked language modeling\n",
    "\n",
    "    <Tip>\n",
    "\n",
    "    This collator relies on details of the implementation of subword tokenization by [`BertTokenizer`], specifically\n",
    "    that subword tokens are prefixed with *##*. For tokenizers that do not adhere to this scheme, this collator will\n",
    "    produce an output that is roughly equivalent to [`.DataCollatorForLanguageModeling`].\n",
    "\n",
    "    </Tip>\"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        emotions = ['기쁨', '기대', '믿음', '놀람', '혐오', '공포', '분노', '슬픔']\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            input_ids = [e[\"input_ids\"] for e in examples]\n",
    "        else:\n",
    "            input_ids = examples\n",
    "            examples = [{\"input_ids\": e} for e in examples]\n",
    "\n",
    "        batch_input = _torch_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "\n",
    "        mask_labels = []\n",
    "        for e in examples:\n",
    "            ref_tokens = []\n",
    "            for id in tolist(e[\"input_ids\"]):\n",
    "                token = self.tokenizer._convert_id_to_token(id)\n",
    "                ref_tokens.append(token)\n",
    "\n",
    "            # For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-> [喜，##欢]\n",
    "            if \"chinese_ref\" in e:\n",
    "                ref_pos = tolist(e[\"chinese_ref\"])\n",
    "                len_seq = len(e[\"input_ids\"])\n",
    "                for i in range(len_seq):\n",
    "                    if i in ref_pos:\n",
    "                        ref_tokens[i] = \"##\" + ref_tokens[i]\n",
    "            mask_labels.append(self._whole_word_mask(ref_tokens, emotions))\n",
    "        batch_mask = _torch_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        inputs, labels = self.torch_mask_tokens(batch_input, batch_mask)\n",
    "        return {\"input_ids\": inputs, \"labels\": labels}\n",
    "\n",
    "    def tf_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        import tensorflow as tf\n",
    "\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            input_ids = [e[\"input_ids\"] for e in examples]\n",
    "        else:\n",
    "            input_ids = examples\n",
    "            examples = [{\"input_ids\": e} for e in examples]\n",
    "\n",
    "        batch_input = _tf_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "\n",
    "        mask_labels = []\n",
    "        for e in examples:\n",
    "            ref_tokens = []\n",
    "            for id in tolist(e[\"input_ids\"]):\n",
    "                token = self.tokenizer._convert_id_to_token(id)\n",
    "                ref_tokens.append(token)\n",
    "\n",
    "            # For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-> [喜，##欢]\n",
    "            if \"chinese_ref\" in e:\n",
    "                ref_pos = tolist(e[\"chinese_ref\"])\n",
    "                len_seq = len(e[\"input_ids\"])\n",
    "                for i in range(len_seq):\n",
    "                    if i in ref_pos:\n",
    "                        ref_tokens[i] = \"##\" + ref_tokens[i]\n",
    "            mask_labels.append(self._whole_word_mask(ref_tokens, emotions))\n",
    "        batch_mask = _tf_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        inputs, labels = self.tf_mask_tokens(tf.cast(batch_input, tf.int64), batch_mask)\n",
    "        return {\"input_ids\": inputs, \"labels\": labels}\n",
    "\n",
    "    def numpy_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        emotions = ['기쁨', '기대', '믿음', '놀람', '혐오', '공포', '분노', '슬픔']\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            input_ids = [e[\"input_ids\"] for e in examples]\n",
    "        else:\n",
    "            input_ids = examples\n",
    "            examples = [{\"input_ids\": e} for e in examples]\n",
    "\n",
    "        batch_input = _numpy_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "\n",
    "        mask_labels = []\n",
    "        for e in examples:\n",
    "            ref_tokens = []\n",
    "            for id in tolist(e[\"input_ids\"]):\n",
    "                token = self.tokenizer._convert_id_to_token(id)\n",
    "                ref_tokens.append(token)\n",
    "\n",
    "            # For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-> [喜，##欢]\n",
    "            if \"chinese_ref\" in e:\n",
    "                ref_pos = tolist(e[\"chinese_ref\"])\n",
    "                len_seq = len(e[\"input_ids\"])\n",
    "                for i in range(len_seq):\n",
    "                    if i in ref_pos:\n",
    "                        ref_tokens[i] = \"##\" + ref_tokens[i]\n",
    "            mask_labels.append(self._whole_word_mask(ref_tokens, emotions))\n",
    "        batch_mask = _numpy_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        inputs, labels = self.numpy_mask_tokens(batch_input, batch_mask)\n",
    "        return {\"input_ids\": inputs, \"labels\": labels}\n",
    "\n",
    "    def _whole_word_mask(self, input_tokens: List[str], max_predictions=192):\n",
    "        emotions = ['기쁨', '기대', '믿음', '놀람', '혐오', '공포', '분노', '슬픔']\n",
    "        \"\"\"\n",
    "        Get 0/1 labels for masked tokens with whole word mask proxy\n",
    "        \"\"\"\n",
    "        if not isinstance(self.tokenizer, (ElectraTokenizer, BertTokenizerFast)):\n",
    "            warnings.warn(\n",
    "                \"DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. \"\n",
    "                \"Please refer to the documentation for more information.\"\n",
    "            )\n",
    "\n",
    "        cand_indexes = []\n",
    "        for i, token in enumerate(input_tokens):\n",
    "            if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "                continue\n",
    "\n",
    "            if len(cand_indexes) >= 1 and token.startswith(\"##\"):\n",
    "                cand_indexes[-1].append(i)\n",
    "            else:\n",
    "                cand_indexes.append([i])\n",
    "\n",
    "        random.shuffle(cand_indexes)\n",
    "        num_to_predict = min(max_predictions, max(1, int(round(len(input_tokens) * self.mlm_probability))))\n",
    "        masked_lms = []\n",
    "        covered_indexes = set()\n",
    "        for index_set in cand_indexes:\n",
    "            if len(masked_lms) >= num_to_predict:\n",
    "                break\n",
    "            # If adding a whole-word mask would exceed the maximum number of\n",
    "            # predictions, then just skip this candidate.\n",
    "            if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "                continue\n",
    "            is_any_index_covered = False\n",
    "            for index in index_set:\n",
    "                if index in covered_indexes:\n",
    "                    is_any_index_covered = True\n",
    "                    break\n",
    "            if is_any_index_covered:\n",
    "                continue\n",
    "            for index in index_set:\n",
    "                covered_indexes.add(index)\n",
    "                masked_lms.append(index)\n",
    "\n",
    "        if len(covered_indexes) != len(masked_lms):\n",
    "            raise ValueError(\"Length of covered_indexes is not equal to length of masked_lms.\")\n",
    "        mask_labels = [1 if i in covered_indexes else 0 for i in range(len(input_tokens))]\n",
    "        return mask_labels\n",
    "\n",
    "    def torch_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n",
    "        'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the\"\n",
    "                \" --mlm flag if you want to use this tokenizer.\"\n",
    "            )\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "\n",
    "        probability_matrix = mask_labels\n",
    "\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "\n",
    "        masked_indices = probability_matrix.bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "\n",
    "    def tf_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n",
    "        'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n",
    "        \"\"\"\n",
    "        import tensorflow as tf\n",
    "\n",
    "        input_shape = tf.shape(inputs)\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the\"\n",
    "                \" --mlm flag if you want to use this tokenizer.\"\n",
    "            )\n",
    "        labels = tf.identity(inputs)\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "\n",
    "        masked_indices = tf.cast(mask_labels, tf.bool)\n",
    "\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels\n",
    "        ]\n",
    "        masked_indices = masked_indices & ~tf.cast(special_tokens_mask, dtype=tf.bool)\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = inputs == self.tokenizer.pad_token_id\n",
    "            masked_indices = masked_indices & ~padding_mask\n",
    "\n",
    "        # Replace unmasked indices with -100 in the labels since we only compute loss on masked tokens\n",
    "        labels = tf.where(masked_indices, inputs, -100)\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = self.tf_bernoulli(input_shape, 0.8) & masked_indices\n",
    "\n",
    "        inputs = tf.where(indices_replaced, self.tokenizer.mask_token_id, inputs)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = self.tf_bernoulli(input_shape, 0.5) & masked_indices & ~indices_replaced\n",
    "        random_words = tf.random.uniform(input_shape, maxval=len(self.tokenizer), dtype=tf.int64)\n",
    "        inputs = tf.where(indices_random, random_words, inputs)\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "\n",
    "    def numpy_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n",
    "        'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n",
    "        \"\"\"\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the\"\n",
    "                \" --mlm flag if you want to use this tokenizer.\"\n",
    "            )\n",
    "        labels = np.copy(inputs)\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "\n",
    "        masked_indices = mask_labels.astype(bool)\n",
    "\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        masked_indices[np.array(special_tokens_mask, dtype=bool)] = 0\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels == self.tokenizer.pad_token_id\n",
    "            masked_indices[padding_mask] = 0\n",
    "\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = np.random.binomial(1, 0.8, size=labels.shape).astype(bool) & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        # indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        indices_random = (\n",
    "            np.random.binomial(1, 0.5, size=labels.shape).astype(bool) & masked_indices & ~indices_replaced\n",
    "        )\n",
    "        random_words = np.random.randint(low=0, high=len(self.tokenizer), size=labels.shape, dtype=np.int64)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSOP(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Data collator used for sentence order prediction task.\n",
    "\n",
    "    - collates batches of tensors, honoring their tokenizer's pad_token\n",
    "    - preprocesses batches for both masked language modeling and sentence order prediction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        warnings.warn(\n",
    "            \"DataCollatorForSOP is deprecated and will be removed in a future version, you can now use \"\n",
    "            \"DataCollatorForLanguageModeling instead.\",\n",
    "            FutureWarning,\n",
    "        )\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        import torch\n",
    "        from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "        input_ids = [example[\"input_ids\"] for example in examples]\n",
    "        input_ids = _torch_collate_batch(input_ids, self.tokenizer)\n",
    "        input_ids, labels, attention_mask = self.mask_tokens(input_ids)\n",
    "\n",
    "        token_type_ids = [example[\"token_type_ids\"] for example in examples]\n",
    "        # size of segment_ids varied because randomness, padding zero to the end as the original implementation\n",
    "        token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "\n",
    "        sop_label_list = [example[\"sentence_order_label\"] for example in examples]\n",
    "        sentence_order_label = torch.stack(sop_label_list)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"sentence_order_label\": sentence_order_label,\n",
    "        }\n",
    "\n",
    "    def mask_tokens(self, inputs: Any) -> Tuple[Any, Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels/attention_mask for masked language modeling: 80% MASK, 10% random, 10%\n",
    "        original. N-gram not applied yet.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the\"\n",
    "                \" --mlm flag if you want to use this tokenizer.\"\n",
    "            )\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        # probability be `1` (masked), however in albert model attention mask `0` means masked, revert the value\n",
    "        attention_mask = (~masked_indices).float()\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            attention_padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            attention_mask.masked_fill_(attention_padding_mask, value=1.0)\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens, -100 is default for CE compute\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels, attention_mask\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForPermutationLanguageModeling(DataCollatorMixin):\n",
    "    \"\"\"\n",
    "    Data collator used for permutation language modeling.\n",
    "\n",
    "    - collates batches of tensors, honoring their tokenizer's pad_token\n",
    "    - preprocesses batches for permutation language modeling with procedures specific to XLNet\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    plm_probability: float = 1 / 6\n",
    "    max_span_length: int = 5  # maximum length of a span of masked tokens\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            examples = [e[\"input_ids\"] for e in examples]\n",
    "        batch = _torch_collate_batch(examples, self.tokenizer)\n",
    "        inputs, perm_mask, target_mapping, labels = self.torch_mask_tokens(batch)\n",
    "        return {\"input_ids\": inputs, \"perm_mask\": perm_mask, \"target_mapping\": target_mapping, \"labels\": labels}\n",
    "\n",
    "    def tf_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            examples = [e[\"input_ids\"] for e in examples]\n",
    "        batch = _tf_collate_batch(examples, self.tokenizer)\n",
    "        inputs, perm_mask, target_mapping, labels = self.tf_mask_tokens(batch)\n",
    "        return {\"input_ids\": inputs, \"perm_mask\": perm_mask, \"target_mapping\": target_mapping, \"labels\": labels}\n",
    "\n",
    "    def numpy_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            examples = [e[\"input_ids\"] for e in examples]\n",
    "        batch = _numpy_collate_batch(examples, self.tokenizer)\n",
    "        inputs, perm_mask, target_mapping, labels = self.numpy_mask_tokens(batch)\n",
    "        return {\"input_ids\": inputs, \"perm_mask\": perm_mask, \"target_mapping\": target_mapping, \"labels\": labels}\n",
    "\n",
    "    def torch_mask_tokens(self, inputs: Any) -> Tuple[Any, Any, Any, Any]:\n",
    "        \"\"\"\n",
    "        The masked tokens to be predicted for a particular sequence are determined by the following algorithm:\n",
    "\n",
    "            0. Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).\n",
    "            1. Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)\n",
    "            2. Reserve a context of length `context_length = span_length / plm_probability` to surround span to be\n",
    "               masked\n",
    "            3. Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length -\n",
    "               span_length]` and mask tokens `start_index:start_index + span_length`\n",
    "            4. Set `cur_len = cur_len + context_length`. If `cur_len < max_len` (i.e. there are tokens remaining in the\n",
    "               sequence to be processed), repeat from Step 1.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for permutation language modeling.\"\n",
    "                \" Please add a mask token if you want to use this tokenizer.\"\n",
    "            )\n",
    "\n",
    "        if inputs.size(1) % 2 != 0:\n",
    "            raise ValueError(\n",
    "                \"This collator requires that sequence lengths be even to create a leakage-free perm_mask. Please see\"\n",
    "                \" relevant comments in source code for details.\"\n",
    "            )\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        # Creating the mask and target_mapping tensors\n",
    "        masked_indices = torch.full(labels.shape, 0, dtype=torch.bool)\n",
    "        target_mapping = torch.zeros((labels.size(0), labels.size(1), labels.size(1)), dtype=torch.float32)\n",
    "\n",
    "        for i in range(labels.size(0)):\n",
    "            # Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).\n",
    "            cur_len = 0\n",
    "            max_len = labels.size(1)\n",
    "\n",
    "            while cur_len < max_len:\n",
    "                # Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)\n",
    "                span_length = torch.randint(1, self.max_span_length + 1, (1,)).item()\n",
    "                # Reserve a context of length `context_length = span_length / plm_probability` to surround the span to be masked\n",
    "                context_length = int(span_length / self.plm_probability)\n",
    "                # Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`\n",
    "                start_index = cur_len + torch.randint(context_length - span_length + 1, (1,)).item()\n",
    "                masked_indices[i, start_index : start_index + span_length] = 1\n",
    "                # Set `cur_len = cur_len + context_length`\n",
    "                cur_len += context_length\n",
    "\n",
    "            # Since we're replacing non-masked tokens with -100 in the labels tensor instead of skipping them altogether,\n",
    "            # the i-th predict corresponds to the i-th token.\n",
    "            target_mapping[i] = torch.eye(labels.size(1))\n",
    "\n",
    "        special_tokens_mask = torch.tensor(\n",
    "            [self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()],\n",
    "            dtype=torch.bool,\n",
    "        )\n",
    "        masked_indices.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            masked_indices.masked_fill_(padding_mask, value=0.0)\n",
    "\n",
    "        # Mask indicating non-functional tokens, where functional tokens are [SEP], [CLS], padding, etc.\n",
    "        non_func_mask = ~(padding_mask | special_tokens_mask)\n",
    "\n",
    "        inputs[masked_indices] = self.tokenizer.mask_token_id\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        perm_mask = torch.zeros((labels.size(0), labels.size(1), labels.size(1)), dtype=torch.float32)\n",
    "\n",
    "        for i in range(labels.size(0)):\n",
    "            # Generate permutation indices i.e. sample a random factorisation order for the sequence. This will\n",
    "            # determine which tokens a given token can attend to (encoded in `perm_mask`).\n",
    "            # Note: Length of token sequence being permuted has to be less than or equal to reused sequence length\n",
    "            # (see documentation for `mems`), otherwise information may leak through due to reuse. In this implementation,\n",
    "            # we assume that reused length is half of sequence length and permutation length is equal to reused length.\n",
    "            # This requires that the sequence length be even.\n",
    "\n",
    "            # Create a linear factorisation order\n",
    "            perm_index = torch.arange(labels.size(1))\n",
    "            # Split this into two halves, assuming that half the sequence is reused each time\n",
    "            perm_index = perm_index.reshape((-1, labels.size(1) // 2)).transpose(0, 1)\n",
    "            # Permute the two halves such that they do not cross over\n",
    "            perm_index = perm_index[torch.randperm(labels.size(1) // 2)]\n",
    "            # Flatten this out into the desired permuted factorisation order\n",
    "            perm_index = torch.flatten(perm_index.transpose(0, 1))\n",
    "            # Set the permutation indices of non-masked (non-functional) tokens to the\n",
    "            # smallest index (-1) so that:\n",
    "            # (1) They can be seen by all other positions\n",
    "            # (2) They cannot see masked positions, so there won't be information leak\n",
    "            perm_index.masked_fill_(~masked_indices[i] & non_func_mask[i], -1)\n",
    "            # The logic for whether the i-th token can attend on the j-th token based on the factorisation order:\n",
    "            # 0 (can attend): If perm_index[i] > perm_index[j] or j is neither masked nor a functional token\n",
    "            # 1 (cannot attend): If perm_index[i] <= perm_index[j] and j is either masked or a functional token\n",
    "            perm_mask[i] = (\n",
    "                perm_index.reshape((labels.size(1), 1)) <= perm_index.reshape((1, labels.size(1)))\n",
    "            ) & masked_indices[i]\n",
    "\n",
    "        return inputs.long(), perm_mask, target_mapping, labels.long()\n",
    "\n",
    "    def tf_mask_tokens(self, inputs: Any) -> Tuple[Any, Any, Any, Any]:\n",
    "        \"\"\"\n",
    "        The masked tokens to be predicted for a particular sequence are determined by the following algorithm:\n",
    "\n",
    "            0. Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).\n",
    "            1. Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)\n",
    "            2. Reserve a context of length `context_length = span_length / plm_probability` to surround span to be\n",
    "               masked\n",
    "            3. Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length -\n",
    "               span_length]` and mask tokens `start_index:start_index + span_length`\n",
    "            4. Set `cur_len = cur_len + context_length`. If `cur_len < max_len` (i.e. there are tokens remaining in the\n",
    "               sequence to be processed), repeat from Step 1.\n",
    "        \"\"\"\n",
    "        import tensorflow as tf\n",
    "\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for permutation language modeling.\"\n",
    "                \" Please add a mask token if you want to use this tokenizer.\"\n",
    "            )\n",
    "\n",
    "        if tf.shape(inputs)[1] % 2 != 0:\n",
    "            raise ValueError(\n",
    "                \"This collator requires that sequence lengths be even to create a leakage-free perm_mask. Please see\"\n",
    "                \" relevant comments in source code for details.\"\n",
    "            )\n",
    "\n",
    "        labels = tf.identity(inputs)\n",
    "        # Creating the mask and target_mapping tensors\n",
    "        masked_indices = np.full(labels.shape.as_list(), 0, dtype=bool)\n",
    "        labels_shape = tf.shape(labels)\n",
    "        target_mapping = np.zeros((labels_shape[0], labels_shape[1], labels_shape[1]), dtype=np.float32)\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            # Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).\n",
    "            cur_len = 0\n",
    "            max_len = tf.shape(labels)[1]\n",
    "\n",
    "            while cur_len < max_len:\n",
    "                # Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)\n",
    "                span_length = randint(1, self.max_span_length + 1)\n",
    "                # Reserve a context of length `context_length = span_length / plm_probability` to surround the span to be masked\n",
    "                context_length = int(span_length / self.plm_probability)\n",
    "                # Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`\n",
    "                start_index = cur_len + randint(0, context_length - span_length + 1)\n",
    "                masked_indices[i, start_index : start_index + span_length] = 1\n",
    "                # Set `cur_len = cur_len + context_length`\n",
    "                cur_len += context_length\n",
    "\n",
    "            # Since we're replacing non-masked tokens with -100 in the labels tensor instead of skipping them altogether,\n",
    "            # the i-th predict corresponds to the i-th token.\n",
    "            target_mapping[i] = np.eye(labels_shape[1])\n",
    "        masked_indices = tf.cast(tf.convert_to_tensor(masked_indices), dtype=tf.bool)\n",
    "        target_mapping = tf.convert_to_tensor(target_mapping)\n",
    "        special_tokens_mask = tf.convert_to_tensor(\n",
    "            [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)\n",
    "                for val in labels.numpy().tolist()\n",
    "            ],\n",
    "        )\n",
    "        special_tokens_mask = tf.cast(special_tokens_mask, dtype=tf.bool)\n",
    "        masked_indices = masked_indices & ~special_tokens_mask\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels == self.tokenizer.pad_token_id\n",
    "            masked_indices = masked_indices & ~padding_mask\n",
    "\n",
    "        # Mask indicating non-functional tokens, where functional tokens are [SEP], [CLS], padding, etc.\n",
    "        non_func_mask = ~(padding_mask | special_tokens_mask)\n",
    "\n",
    "        inputs = tf.where(masked_indices, self.tokenizer.mask_token_id, inputs)\n",
    "        labels = tf.where(masked_indices, labels, -100)  # We only compute loss on masked tokens\n",
    "\n",
    "        perm_mask = []\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            # Generate permutation indices i.e. sample a random factorisation order for the sequence. This will\n",
    "            # determine which tokens a given token can attend to (encoded in `perm_mask`).\n",
    "            # Note: Length of token sequence being permuted has to be less than or equal to reused sequence length\n",
    "            # (see documentation for `mems`), otherwise information may leak through due to reuse. In this implementation,\n",
    "            # we assume that reused length is half of sequence length and permutation length is equal to reused length.\n",
    "            # This requires that the sequence length be even.\n",
    "\n",
    "            # Create a linear factorisation order\n",
    "            # tf.range is the equivalent of torch.arange\n",
    "            perm_index = tf.range(labels_shape[1])\n",
    "            # Split this into two halves, assuming that half the sequence is reused each time\n",
    "            perm_index = tf.transpose(tf.reshape(perm_index, (-1, labels_shape[1] // 2)))\n",
    "            # Permute the two halves such that they do not cross over\n",
    "            perm_index = tf.random.shuffle(perm_index)  # Shuffles along the first dimension\n",
    "            # Flatten this out into the desired permuted factorisation order\n",
    "            perm_index = tf.reshape(tf.transpose(perm_index), (-1,))\n",
    "            # Set the permutation indices of non-masked (non-functional) tokens to the\n",
    "            # smallest index (-1) so that:\n",
    "            # (1) They can be seen by all other positions\n",
    "            # (2) They cannot see masked positions, so there won't be information leak\n",
    "            perm_index = tf.where(~masked_indices[i] & non_func_mask[i], -1, perm_index)\n",
    "            # The logic for whether the i-th token can attend on the j-th token based on the factorisation order:\n",
    "            # 0 (can attend): If perm_index[i] > perm_index[j] or j is neither masked nor a functional token\n",
    "            # 1 (cannot attend): If perm_index[i] <= perm_index[j] and j is either masked or a functional token\n",
    "            perm_mask.append(\n",
    "                (tf.reshape(perm_index, (labels_shape[1], 1)) <= tf.reshape(perm_index, (1, labels_shape[1])))\n",
    "                & masked_indices[i]\n",
    "            )\n",
    "        perm_mask = tf.stack(perm_mask, axis=0)\n",
    "\n",
    "        return tf.cast(inputs, tf.int64), tf.cast(perm_mask, tf.float32), target_mapping, tf.cast(labels, tf.int64)\n",
    "\n",
    "    def numpy_mask_tokens(self, inputs: Any) -> Tuple[Any, Any, Any, Any]:\n",
    "        \"\"\"\n",
    "        The masked tokens to be predicted for a particular sequence are determined by the following algorithm:\n",
    "\n",
    "            0. Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).\n",
    "            1. Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)\n",
    "            2. Reserve a context of length `context_length = span_length / plm_probability` to surround span to be\n",
    "               masked\n",
    "            3. Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length -\n",
    "               span_length]` and mask tokens `start_index:start_index + span_length`\n",
    "            4. Set `cur_len = cur_len + context_length`. If `cur_len < max_len` (i.e. there are tokens remaining in the\n",
    "               sequence to be processed), repeat from Step 1.\n",
    "        \"\"\"\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for permutation language modeling.\"\n",
    "                \" Please add a mask token if you want to use this tokenizer.\"\n",
    "            )\n",
    "\n",
    "        if inputs.shape[1] % 2 != 0:\n",
    "            raise ValueError(\n",
    "                \"This collator requires that sequence lengths be even to create a leakage-free perm_mask. Please see\"\n",
    "                \" relevant comments in source code for details.\"\n",
    "            )\n",
    "\n",
    "        labels = np.copy(inputs)\n",
    "        # Creating the mask and target_mapping tensors\n",
    "        masked_indices = np.full(labels.shape, 0, dtype=bool)\n",
    "        target_mapping = np.zeros((labels.shape[0], labels.shape[1], labels.shape[1]), dtype=np.float32)\n",
    "\n",
    "        for i in range(labels.shape[0]):\n",
    "            # Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).\n",
    "            cur_len = 0\n",
    "            max_len = labels.shape[1]\n",
    "\n",
    "            while cur_len < max_len:\n",
    "                # Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be masked)\n",
    "                span_length = randint(1, self.max_span_length + 1)\n",
    "                # Reserve a context of length `context_length = span_length / plm_probability` to surround the span to be masked\n",
    "                context_length = int(span_length / self.plm_probability)\n",
    "                # Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`\n",
    "                start_index = cur_len + randint(0, context_length - span_length + 1)\n",
    "                masked_indices[i, start_index : start_index + span_length] = 1\n",
    "                # Set `cur_len = cur_len + context_length`\n",
    "                cur_len += context_length\n",
    "\n",
    "            # Since we're replacing non-masked tokens with -100 in the labels tensor instead of skipping them altogether,\n",
    "            # the i-th predict corresponds to the i-th token.\n",
    "            target_mapping[i] = np.eye(labels.shape[1])\n",
    "\n",
    "        special_tokens_mask = np.array(\n",
    "            [self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()],\n",
    "            dtype=bool,\n",
    "        )\n",
    "        masked_indices[special_tokens_mask] = 0\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels == self.tokenizer.pad_token_id\n",
    "            masked_indices[padding_mask] = 0.0\n",
    "\n",
    "        # Mask indicating non-functional tokens, where functional tokens are [SEP], [CLS], padding, etc.\n",
    "        non_func_mask = ~(padding_mask | special_tokens_mask)\n",
    "\n",
    "        inputs[masked_indices] = self.tokenizer.mask_token_id\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        perm_mask = np.zeros((labels.shape[0], labels.shape[1], labels.shape[1]), dtype=np.float32)\n",
    "\n",
    "        for i in range(labels.shape[0]):\n",
    "            # Generate permutation indices i.e. sample a random factorisation order for the sequence. This will\n",
    "            # determine which tokens a given token can attend to (encoded in `perm_mask`).\n",
    "            # Note: Length of token sequence being permuted has to be less than or equal to reused sequence length\n",
    "            # (see documentation for `mems`), otherwise information may leak through due to reuse. In this implementation,\n",
    "            # we assume that reused length is half of sequence length and permutation length is equal to reused length.\n",
    "            # This requires that the sequence length be even.\n",
    "\n",
    "            # Create a linear factorisation order\n",
    "            perm_index = np.arange(labels.shape[1])\n",
    "            # Split this into two halves, assuming that half the sequence is reused each time\n",
    "            perm_index = perm_index.reshape((-1, labels.shape[1] // 2)).T\n",
    "            # Permute the two halves such that they do not cross over\n",
    "            np.random.shuffle(perm_index)\n",
    "            # Flatten this out into the desired permuted factorisation order\n",
    "            perm_index = perm_index.T.flatten()\n",
    "            # Set the permutation indices of non-masked (non-functional) tokens to the\n",
    "            # smallest index (-1) so that:\n",
    "            # (1) They can be seen by all other positions\n",
    "            # (2) They cannot see masked positions, so there won't be information leak\n",
    "            perm_index[~masked_indices[i] & non_func_mask[i]] = -1\n",
    "            # The logic for whether the i-th token can attend on the j-th token based on the factorisation order:\n",
    "            # 0 (can attend): If perm_index[i] > perm_index[j] or j is neither masked nor a functional token\n",
    "            # 1 (cannot attend): If perm_index[i] <= perm_index[j] and j is either masked or a functional token\n",
    "            perm_mask[i] = (\n",
    "                perm_index.reshape((labels.shape[1], 1)) <= perm_index.reshape((1, labels.shape[1]))\n",
    "            ) & masked_indices[i]\n",
    "\n",
    "        return inputs.astype(np.int64), perm_mask, target_mapping, labels.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForWholeWordMask(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Data collator used for language modeling that masks entire words.\n",
    "\n",
    "    - collates batches of tensors, honoring their tokenizer's pad_token\n",
    "    - preprocesses batches for masked language modeling\n",
    "\n",
    "    <Tip>\n",
    "\n",
    "    This collator relies on details of the implementation of subword tokenization by [`BertTokenizer`], specifically\n",
    "    that subword tokens are prefixed with *##*. For tokenizers that do not adhere to this scheme, this collator will\n",
    "    produce an output that is roughly equivalent to [`.DataCollatorForLanguageModeling`].\n",
    "\n",
    "    </Tip>\"\"\"\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            input_ids = [e[\"input_ids\"] for e in examples]\n",
    "        else:\n",
    "            input_ids = examples\n",
    "            examples = [{\"input_ids\": e} for e in examples]\n",
    "\n",
    "        batch_input = _torch_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "\n",
    "        mask_labels = []\n",
    "        for e in examples:\n",
    "            ref_tokens = []\n",
    "            for id in tolist(e[\"input_ids\"]):\n",
    "                token = self.tokenizer._convert_id_to_token(id)\n",
    "                ref_tokens.append(token)\n",
    "\n",
    "            # For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-> [喜，##欢]\n",
    "            if \"chinese_ref\" in e:\n",
    "                ref_pos = tolist(e[\"chinese_ref\"])\n",
    "                len_seq = len(e[\"input_ids\"])\n",
    "                for i in range(len_seq):\n",
    "                    if i in ref_pos:\n",
    "                        ref_tokens[i] = \"##\" + ref_tokens[i]\n",
    "            mask_labels.append(self._whole_word_mask(ref_tokens))\n",
    "        batch_mask = _torch_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        inputs, labels = self.torch_mask_tokens(batch_input, batch_mask)\n",
    "        return {\"input_ids\": inputs, \"labels\": labels}\n",
    "\n",
    "    \n",
    "\n",
    "    def _whole_word_mask(self, input_tokens: List[str], max_predictions=192):\n",
    "        \"\"\"\n",
    "        Get 0/1 labels for masked tokens with whole word mask proxy\n",
    "        \"\"\"\n",
    "        if not isinstance(self.tokenizer, (ElectraTokenizer, BertTokenizerFast)):\n",
    "            warnings.warn(\n",
    "                \"DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. \"\n",
    "                \"Please refer to the documentation for more information.\"\n",
    "            )\n",
    "\n",
    "        cand_indexes = []\n",
    "        for i, token in enumerate(input_tokens):\n",
    "            if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "                continue\n",
    "\n",
    "            if len(cand_indexes) >= 1 and token.startswith(\"##\"):\n",
    "                cand_indexes[-1].append(i)\n",
    "            else:\n",
    "                cand_indexes.append([i])\n",
    "\n",
    "        random.shuffle(cand_indexes)\n",
    "        num_to_predict = min(max_predictions, max(1, int(round(len(input_tokens) * self.mlm_probability))))\n",
    "        masked_lms = []\n",
    "        covered_indexes = set()\n",
    "        for index_set in cand_indexes:\n",
    "            if len(masked_lms) >= num_to_predict:\n",
    "                break\n",
    "            # If adding a whole-word mask would exceed the maximum number of\n",
    "            # predictions, then just skip this candidate.\n",
    "            if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "                continue\n",
    "            is_any_index_covered = False\n",
    "            for index in index_set:\n",
    "                if index in covered_indexes:\n",
    "                    is_any_index_covered = True\n",
    "                    break\n",
    "            if is_any_index_covered:\n",
    "                continue\n",
    "            for index in index_set:\n",
    "                covered_indexes.add(index)\n",
    "                masked_lms.append(index)\n",
    "\n",
    "        if len(covered_indexes) != len(masked_lms):\n",
    "            raise ValueError(\"Length of covered_indexes is not equal to length of masked_lms.\")\n",
    "        mask_labels = [1 if i in covered_indexes else 0 for i in range(len(input_tokens))]\n",
    "        return mask_labels\n",
    "\n",
    "    def torch_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n",
    "        'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the\"\n",
    "                \" --mlm flag if you want to use this tokenizer.\"\n",
    "            )\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "\n",
    "        probability_matrix = mask_labels\n",
    "\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "\n",
    "        masked_indices = probability_matrix.bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InputDataClass = NewType(\"InputDataClass\", Any)\n",
    "# DataCollator = NewType(\"DataCollator\", Callable[[List[InputDataClass]], Dict[str, Any]])\n",
    "\n",
    "# @dataclass\n",
    "# class DataCollatorForWholeWordMask:\n",
    "#     tokenizer: PreTrainedTokenizerBase\n",
    "#     mlm_probability: float = 0.15\n",
    "\n",
    "#     def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "#         input_ids = [self.tokenizer(e['prepended_text'], add_special_tokens=True)['input_ids'] for e in examples]\n",
    "#         target_positions = []\n",
    "#         for e in examples:\n",
    "#             if e['target'] is not None:\n",
    "#                 target_positions.append(e['target']['begin'] + 4)  # +4 for [CLS] + 감정 라벨 + [SEP]\n",
    "#             else:\n",
    "#                 target_positions.append(None)\n",
    "\n",
    "#         max_length = max(len(ids) for ids in input_ids)\n",
    "#         input_ids = [ids + [self.tokenizer.pad_token_id] * (max_length - len(ids)) for ids in input_ids]\n",
    "#         input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "#         attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
    "\n",
    "#         labels = input_ids.clone().detach()\n",
    "#         labels[~attention_mask.bool()] = -100\n",
    "\n",
    "#         # 마스킹하지 않을 위치를 표시\n",
    "#         no_mask_positions = torch.zeros_like(input_ids)\n",
    "#         for i, pos in enumerate(target_positions):\n",
    "#             no_mask_positions[i, :4] = 1  # [CLS] + 감정 라벨 + [SEP]\n",
    "#             if pos is not None:\n",
    "#                 no_mask_positions[i, pos] = 1  # 타겟\n",
    "\n",
    "#         # 마스킹 수행\n",
    "#         probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "#         probability_matrix.masked_fill_(no_mask_positions.bool(), value=0.0)\n",
    "#         masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "#         labels[~masked_indices] = -100\n",
    "\n",
    "#         # 80% 마스크, 10% 랜덤, 10% 원래대로\n",
    "#         indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "#         input_ids[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "#         indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "#         random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "#         input_ids[indices_random] = random_words[indices_random]\n",
    "\n",
    "#         return {\n",
    "#             \"input_ids\": input_ids,\n",
    "#             \"labels\": labels,\n",
    "#             \"attention_mask\": attention_mask\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class DataCollatorForWholeWordMask:\n",
    "#     tokenizer: PreTrainedTokenizerBase\n",
    "#     mlm_probability: float = 0.15\n",
    "\n",
    "#     def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "#         print(examples)\n",
    "#         input_ids = [self.tokenizer(e['prepended_text'], add_special_tokens=True)['input_ids'] for e in examples]\n",
    "#         target_positions = []\n",
    "#         for e in examples:\n",
    "#             if e['target'] is not None and e['target']['begin'] is not None and e['target']['end'] is not None:\n",
    "#                 target_positions.append(e['target']['begin'])\n",
    "#             else:\n",
    "#                 target_positions.append(None)\n",
    "\n",
    "#         max_length = max(len(ids) for ids in input_ids)\n",
    "#         input_ids = [ids + [self.tokenizer.pad_token_id] * (max_length - len(ids)) for ids in input_ids]\n",
    "#         input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "#         attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
    "\n",
    "#         labels = input_ids.clone().detach()\n",
    "#         labels[~attention_mask.bool()] = -100\n",
    "\n",
    "#         # 마스킹하지 않을 위치를 표시\n",
    "#         no_mask_positions = torch.zeros_like(input_ids)\n",
    "#         for i, pos in enumerate(target_positions):\n",
    "#             no_mask_positions[i, :4] = 1  # [CLS] + 감정 라벨 + [SEP]\n",
    "#             if pos is not None:\n",
    "#                 no_mask_positions[i, pos] = 1  # 타겟\n",
    "\n",
    "#         # 마스킹 수행\n",
    "#         probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "#         probability_matrix.masked_fill_(no_mask_positions.bool(), value=0.0)\n",
    "#         masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "#         labels[~masked_indices] = -100\n",
    "\n",
    "#         # 80% 마스크, 10% 랜덤, 10% 원래대로\n",
    "#         indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "#         input_ids[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "#         indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "#         random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "#         input_ids[indices_random] = random_words[indices_random]\n",
    "\n",
    "#         return {\n",
    "#             \"input_ids\": input_ids,\n",
    "#             \"labels\": labels,\n",
    "#             \"attention_mask\": attention_mask\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        prepended_text = item['prepended_text']\n",
    "        target = item.get('target', None)\n",
    "        encoding = self.tokenizer(prepended_text, add_special_tokens=True, truncation=True, padding='max_length', max_length=192)\n",
    "        encoding['target'] = target  # target 정보 추가\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 10223, 3755, 18, 18, 18, 7289, 3325, 4230, 4825, 6395, 8757, 4232, 4594, 18, 18, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'target': {'form': '준프샤', 'begin': 11, 'end': 14}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = CustomDataset(prepended_data, tokenizer)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = CustomDataset(dev_prepended_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CustomDataCollatorForWholeWordMask(DataCollatorForWholeWordMask):\n",
    "    \"\"\"\n",
    "    Data collator used for language modeling that masks entire words.\n",
    "\n",
    "    - collates batches of tensors, honoring their tokenizer's pad_token\n",
    "    - preprocesses batches for masked language modeling\n",
    "\n",
    "    <Tip>\n",
    "\n",
    "    This collator relies on details of the implementation of subword tokenization by [`BertTokenizer`], specifically\n",
    "    that subword tokens are prefixed with *##*. For tokenizers that do not adhere to this scheme, this collator will\n",
    "    produce an output that is roughly equivalent to [`.DataCollatorForLanguageModeling`].\n",
    "\n",
    "    </Tip>\"\"\"\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            input_ids = [e[\"input_ids\"] for e in examples]\n",
    "        else:\n",
    "            input_ids = examples\n",
    "            examples = [{\"input_ids\": e} for e in examples]\n",
    "\n",
    "        batch_input = _torch_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "\n",
    "        targets = [e.get(\"target\", None) for e in examples]\n",
    "\n",
    "        mask_labels = []\n",
    "        for i, e in enumerate(examples):\n",
    "            ref_tokens = []\n",
    "            for id in tolist(e[\"input_ids\"]):\n",
    "                token = self.tokenizer._convert_id_to_token(id)\n",
    "                ref_tokens.append(token)\n",
    "\n",
    "            mask_labels.append(self._whole_word_mask(ref_tokens, emotions, targets[i]))\n",
    "        batch_mask = _torch_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        inputs, labels = self.torch_mask_tokens(batch_input, batch_mask)\n",
    "        return {\"input_ids\": inputs, \"labels\": labels}\n",
    "\n",
    "\n",
    "    def _whole_word_mask(self, input_tokens: List[str], emotions: List[str], target: List[Dict[str, Any]], max_predictions=192):\n",
    "        \"\"\"\n",
    "        Get 0/1 labels for masked tokens with whole word mask proxy\n",
    "        \"\"\"\n",
    "        if not isinstance(self.tokenizer, (ElectraTokenizer, ElectraTokenizerFast)):\n",
    "            warnings.warn(\n",
    "                \"DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. \"\n",
    "                \"Please refer to the documentation for more information.\"\n",
    "            )\n",
    "        emotions = ['기쁨', '기대', '믿음', '놀람', '혐오', '공포', '분노', '슬픔']\n",
    "        emotion_ids = [self.tokenizer.convert_tokens_to_ids(e) for e in emotions]\n",
    "        cand_indexes = []\n",
    "        for i, token in enumerate(input_tokens):\n",
    "            token_id = self.tokenizer.convert_tokens_to_ids(token)\n",
    "            if token == \"[CLS]\" or token == \"[SEP]\" or token_id in emotion_ids:  # 여기 수정\n",
    "                continue\n",
    "\n",
    "            if target is not None and target['begin'] is not None and target['end'] is not None:\n",
    "                if i >= target['begin'] and i <= target['end']:  # target에 해당하는 토큰은 건너뛴다\n",
    "                    continue\n",
    "\n",
    "            if len(cand_indexes) >= 1 and token.startswith(\"##\"):\n",
    "                cand_indexes[-1].append(i)\n",
    "            else:\n",
    "                cand_indexes.append([i])\n",
    "\n",
    "        random.shuffle(cand_indexes)\n",
    "        num_to_predict = min(max_predictions, max(1, int(round(len(input_tokens) * self.mlm_probability))))\n",
    "        masked_lms = []\n",
    "        covered_indexes = set()\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        for index_set in cand_indexes:\n",
    "            if len(masked_lms) >= num_to_predict:\n",
    "                break\n",
    "            # If adding a whole-word mask would exceed the maximum number of\n",
    "            # predictions, then just skip this candidate.\n",
    "            if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "                continue\n",
    "            is_any_index_covered = False\n",
    "            for index in index_set:\n",
    "                if index in covered_indexes:\n",
    "                    is_any_index_covered = True\n",
    "                    break\n",
    "            if is_any_index_covered:\n",
    "                continue\n",
    "            for index in index_set:\n",
    "                covered_indexes.add(index)\n",
    "                masked_lms.append(index)\n",
    "\n",
    "        if len(covered_indexes) != len(masked_lms):\n",
    "            raise ValueError(\"Length of covered_indexes is not equal to length of masked_lms.\")\n",
    "        mask_labels = [1 if i in covered_indexes else 0 for i in range(len(input_tokens))]\n",
    "        return mask_labels\n",
    "\n",
    "    def torch_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n",
    "        'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the\"\n",
    "                \" --mlm flag if you want to use this tokenizer.\"\n",
    "            )\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "\n",
    "        probability_matrix = mask_labels\n",
    "\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "\n",
    "        masked_indices = probability_matrix.bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = CustomDataCollatorForWholeWordMask(tokenizer=tokenizer)\n",
    "\n",
    "# # 데이터 콜레이터 테스트\n",
    "# batch = data_collator(prepended_data[:10])\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer)\n",
    "\n",
    "# # 데이터 콜레이터 테스트\n",
    "# batch = data_collator(prepended_data[:10])\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output_ELECTRA_prepend_fine_tuning_2e-4\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=2e-4,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: {'input_ids': [2, 10223, 3755, 18, 18, 18, 7289, 3325, 4230, 4825, 6395, 8757, 4232, 4594, 18, 18, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'target': {'form': '준프샤', 'begin': 11, 'end': 14}}\n",
      "Length of Train Dataset: 37906\n",
      "Sample from Train Dataset: {'input_ids': [2, 10223, 3755, 18, 18, 18, 7289, 3325, 4230, 4825, 6395, 8757, 4232, 4594, 18, 18, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'target': {'form': '준프샤', 'begin': 11, 'end': 14}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Dataset:\", trainer.train_dataset[0])\n",
    "print(\"Length of Train Dataset:\", len(trainer.train_dataset))\n",
    "print(\"Sample from Train Dataset:\", trainer.train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='23700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    4/23700 00:00 < 47:22, 8.34 it/s, Epoch 0.00/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eojin_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
