{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizerFast, ElectraForMaskedLM, AdamW\n",
    "import random\n",
    "import logging\n",
    "import json\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "import torch.distributed as dist\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/nlpgpu9/ellt/eojin/EA/nikluge-ea-2023-train_수정_중복제거.jsonl\", 'r') as file:\n",
    "    data = []\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/nlpgpu9/ellt/eojin/EA/nikluge-ea-2023-dev_수정.jsonl\", 'r') as file:\n",
    "    dev_data = []\n",
    "    for line in file:\n",
    "        dev_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForMaskedLM were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['generator_predictions.dense.weight', 'generator_predictions.dense.bias', 'generator_predictions.LayerNorm.bias', 'generator_lm_head.bias', 'generator_predictions.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "model = ElectraForMaskedLM.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 하 [MASK].. 근데 준프샤 너무 고소각임.. [MASK] [SEP]\n"
     ]
    }
   ],
   "source": [
    "def mask_text_with_tokenizer(text, target_begin, target_end, tokenizer, mask_prob=0.15):\n",
    "    # 전체 문장 토큰화\n",
    "    encoding = tokenizer.encode_plus(text, add_special_tokens=True)\n",
    "    tokens = encoding['input_ids']\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "    # 문장에서 타겟 찾기\n",
    "    target_tokens = []\n",
    "    if target_begin is not None and target_end is not None:\n",
    "        target_text = text[target_begin:target_end]\n",
    "        target_tokens = tokenizer.tokenize(target_text)\n",
    "\n",
    "    masked_tokens = []\n",
    "    skip_count = 0\n",
    "    \n",
    "    # 전체 15% 마스킹(반올림). 적어도 한 개는 마스킹\n",
    "    num_to_mask = max(1, round(len(tokens) * mask_prob))\n",
    "    \n",
    "    # 타겟 아닌 것만 마스킹 예약\n",
    "    available_indices = [i for i in range(1, len(tokens) - 1)\n",
    "                         if tokens[i:i+len(target_tokens)] != target_tokens]\n",
    "    \n",
    "    # 마스킹할 토큰 랜덤 선택\n",
    "    indices_to_mask = random.sample(available_indices, min(num_to_mask, len(available_indices)))\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if skip_count > 0:\n",
    "            skip_count -= 1\n",
    "            continue\n",
    "\n",
    "        if tokens[i:i+len(target_tokens)] == target_tokens:\n",
    "            masked_tokens.extend(target_tokens)\n",
    "            skip_count = len(target_tokens) - 1\n",
    "            continue\n",
    "\n",
    "        if i in indices_to_mask:\n",
    "            masked_tokens.append('[MASK]')\n",
    "        else:\n",
    "            masked_tokens.append(token)\n",
    "    \n",
    "    # 다시 문장으로 바꾸기\n",
    "    detokenized_text = tokenizer.convert_tokens_to_string(masked_tokens)  # [CLS]랑 [SEP]은 제외하려면 (masked_tokens[1:-1]\n",
    "    \n",
    "    return detokenized_text\n",
    "\n",
    "\n",
    "\n",
    "sample_text = \"하... 근데 준프샤 너무 고소각임...\"\n",
    "sample_target_begin = 8\n",
    "sample_target_end = 11\n",
    "print(mask_text_with_tokenizer(sample_text, sample_target_begin, sample_target_end, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(data_list, tokenizer, mask_prob=0.15):\n",
    "    preprocessed_data = []\n",
    "    \n",
    "    for data_point in data_list:\n",
    "        text = data_point['input']['form']\n",
    "        target_begin = data_point['input']['target']['begin']\n",
    "        target_end = data_point['input']['target']['end']     \n",
    "\n",
    "        masked_text = mask_text_with_tokenizer(text, target_begin, target_end, tokenizer, mask_prob)\n",
    "        \n",
    "        preprocessed_data.append({\n",
    "            'id': data_point['id'],\n",
    "            'masked_text': masked_text,\n",
    "            'labels': data_point['output']\n",
    "        })\n",
    "        \n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'nikluge-2023-ea-train-000001',\n",
       "  'masked_text': '[CLS] 하... 근데 준프샤 너무 [MASK]각임... [SEP]',\n",
       "  'labels': {'joy': 'True',\n",
       "   'anticipation': 'False',\n",
       "   'trust': 'False',\n",
       "   'surprise': 'False',\n",
       "   'disgust': 'False',\n",
       "   'fear': 'False',\n",
       "   'anger': 'False',\n",
       "   'sadness': 'False'}},\n",
       " {'id': 'nikluge-2023-ea-train-000002',\n",
       "  'masked_text': '[CLS] 2기였나 지은 [MASK] [MASK]랑 4기 메거진 [MASK] 지금도 읽는데 [SEP]',\n",
       "  'labels': {'joy': 'True',\n",
       "   'anticipation': 'False',\n",
       "   'trust': 'False',\n",
       "   'surprise': 'False',\n",
       "   'disgust': 'False',\n",
       "   'fear': 'False',\n",
       "   'anger': 'False',\n",
       "   'sadness': 'False'}},\n",
       " {'id': 'nikluge-2023-ea-train-000003',\n",
       "  'masked_text': '[CLS] 흐아아아아악 흐아아아아 [MASK] [MASK]악악 [UNK] 손차이가 절케 난다 [MASK] 알고 [MASK] [MASK]엇지만 놀랍다 [MASK]아아악악 [SEP]',\n",
       "  'labels': {'joy': 'False',\n",
       "   'anticipation': 'False',\n",
       "   'trust': 'False',\n",
       "   'surprise': 'True',\n",
       "   'disgust': 'False',\n",
       "   'fear': 'False',\n",
       "   'anger': 'False',\n",
       "   'sadness': 'False'}},\n",
       " {'id': 'nikluge-2023-ea-train-000004',\n",
       "  'masked_text': '',\n",
       "  'labels': {'joy': 'False',\n",
       "   'anticipation': 'True',\n",
       "   'trust': 'False',\n",
       "   'surprise': 'False',\n",
       "   'disgust': 'False',\n",
       "   'fear': 'False',\n",
       "   'anger': 'False',\n",
       "   'sadness': 'False'}},\n",
       " {'id': 'nikluge-2023-ea-train-000005',\n",
       "  'masked_text': '[CLS] [MASK] [MASK]에 티켓팅 공지 뜨 [MASK] 다담주에 티켓팅할 느낌 [SEP]',\n",
       "  'labels': {'joy': 'False',\n",
       "   'anticipation': 'True',\n",
       "   'trust': 'False',\n",
       "   'surprise': 'False',\n",
       "   'disgust': 'False',\n",
       "   'fear': 'False',\n",
       "   'anger': 'False',\n",
       "   'sadness': 'False'}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = preprocess_dataset(data, tokenizer)\n",
    "preprocessed_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'nikluge-2023-ea-dev-000001',\n",
       "  'masked_text': '[CLS] 하,,, [MASK] 내일 옥상 [MASK] [MASK] 하기 전에 표 구하길 기도해주세요 [SEP]',\n",
       "  'labels': {'joy': 'False',\n",
       "   'anticipation': 'True',\n",
       "   'trust': 'False',\n",
       "   'surprise': 'False',\n",
       "   'disgust': 'False',\n",
       "   'fear': 'False',\n",
       "   'anger': 'False',\n",
       "   'sadness': 'False'}},\n",
       " {'id': 'nikluge-2023-ea-dev-000002',\n",
       "  'masked_text': '[CLS] 밴드 사운드 진짜 너무너무 좋다.... [MASK]인 [MASK] 진짜 [SEP]',\n",
       "  'labels': {'joy': 'True',\n",
       "   'anticipation': 'False',\n",
       "   'trust': 'False',\n",
       "   'surprise': 'False',\n",
       "   'disgust': 'False',\n",
       "   'fear': 'False',\n",
       "   'anger': 'False',\n",
       "   'sadness': 'False'}},\n",
       " {'id': 'nikluge-2023-ea-dev-000003',\n",
       "  'masked_text': '[CLS] 칸 태리 너무 풋풋 [MASK] [MASK] 귀여워.. 근데 허리가 한줌 [MASK]셔 & [MASK]s & [SEP]',\n",
       "  'labels': {'joy': 'True',\n",
       "   'anticipation': 'False',\n",
       "   'trust': 'False',\n",
       "   'surprise': 'False',\n",
       "   'disgust': 'False',\n",
       "   'fear': 'False',\n",
       "   'anger': 'False',\n",
       "   'sadness': 'False'}},\n",
       " {'id': 'nikluge-2023-ea-dev-000004',\n",
       "  'masked_text': '[CLS] 미스터초밥왕이 햄스터 [MASK]씨가 [MASK]어 오는 기분 [MASK]다는 더 [MASK]지 싶음.. [SEP]',\n",
       "  'labels': {'joy': 'False',\n",
       "   'anticipation': 'True',\n",
       "   'trust': 'False',\n",
       "   'surprise': 'False',\n",
       "   'disgust': 'False',\n",
       "   'fear': 'False',\n",
       "   'anger': 'False',\n",
       "   'sadness': 'False'}},\n",
       " {'id': 'nikluge-2023-ea-dev-000005',\n",
       "  'masked_text': '[CLS] [UNK] 초연 자첫날 진심 한 마디 안에 멜로디 무시하고 가사 너무 [MASK] 들어간 [MASK] 같 [MASK]서 [MASK]?? [MASK] [MASK] 그 뒤로 그런극 많이 본 것 같아서 이제 놀랍진 [MASK]음 [SEP]',\n",
       "  'labels': {'joy': 'False',\n",
       "   'anticipation': 'False',\n",
       "   'trust': 'False',\n",
       "   'surprise': 'True',\n",
       "   'disgust': 'False',\n",
       "   'fear': 'False',\n",
       "   'anger': 'False',\n",
       "   'sadness': 'False'}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_preprocessed_data = preprocess_dataset(dev_data, tokenizer)\n",
    "dev_preprocessed_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_labels_to_text(preprocessed_data, label_mapping):\n",
    "    prepended_data = []\n",
    "    \n",
    "    for data_point in preprocessed_data:\n",
    "        labels = data_point['labels']\n",
    "        masked_text = data_point['masked_text']\n",
    "        \n",
    "        # 라벨 찾기\n",
    "        prepend_str = ' '.join([f'{label_mapping[label]}' for label, value in labels.items() if value == 'True'])\n",
    "        \n",
    "        # [CLS] 뒤, 문장 앞에 라벨 붙이기\n",
    "        if masked_text.startswith(\"[CLS]\"):\n",
    "            new_text = f\"[CLS] {prepend_str} {masked_text[6:]}\" if prepend_str else masked_text\n",
    "        else:\n",
    "            new_text = f\"{prepend_str} {masked_text}\" if prepend_str else masked_text\n",
    "\n",
    "        # 데이터셋 만들기\n",
    "        prepended_data.append({\n",
    "            'id': data_point['id'],\n",
    "            'prepended_text': new_text,\n",
    "            'labels': labels\n",
    "        })\n",
    "        \n",
    "    return prepended_data\n",
    "\n",
    "# 라벨은 한국어로\n",
    "label_mapping = {\n",
    "    'joy': '기쁨',\n",
    "    'anticipation': '기대',\n",
    "    'trust': '믿음',\n",
    "    'surprise': '놀람',\n",
    "    'disgust': '혐오',\n",
    "    'fear': '공포',\n",
    "    'anger': '분노',\n",
    "    'sadness': '슬픔'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'nikluge-2023-ea-train-000001', 'prepended_text': '[CLS] 기쁨 하... 근데 준프샤 너무 [MASK]각임... [SEP]', 'labels': {'joy': 'True', 'anticipation': 'False', 'trust': 'False', 'surprise': 'False', 'disgust': 'False', 'fear': 'False', 'anger': 'False', 'sadness': 'False'}}, {'id': 'nikluge-2023-ea-train-000002', 'prepended_text': '[CLS] 기쁨 2기였나 지은 [MASK] [MASK]랑 4기 메거진 [MASK] 지금도 읽는데 [SEP]', 'labels': {'joy': 'True', 'anticipation': 'False', 'trust': 'False', 'surprise': 'False', 'disgust': 'False', 'fear': 'False', 'anger': 'False', 'sadness': 'False'}}, {'id': 'nikluge-2023-ea-train-000003', 'prepended_text': '[CLS] 놀람 흐아아아아악 흐아아아아 [MASK] [MASK]악악 [UNK] 손차이가 절케 난다 [MASK] 알고 [MASK] [MASK]엇지만 놀랍다 [MASK]아아악악 [SEP]', 'labels': {'joy': 'False', 'anticipation': 'False', 'trust': 'False', 'surprise': 'True', 'disgust': 'False', 'fear': 'False', 'anger': 'False', 'sadness': 'False'}}, {'id': 'nikluge-2023-ea-train-000004', 'prepended_text': '기대 ', 'labels': {'joy': 'False', 'anticipation': 'True', 'trust': 'False', 'surprise': 'False', 'disgust': 'False', 'fear': 'False', 'anger': 'False', 'sadness': 'False'}}, {'id': 'nikluge-2023-ea-train-000005', 'prepended_text': '[CLS] 기대 [MASK] [MASK]에 티켓팅 공지 뜨 [MASK] 다담주에 티켓팅할 느낌 [SEP]', 'labels': {'joy': 'False', 'anticipation': 'True', 'trust': 'False', 'surprise': 'False', 'disgust': 'False', 'fear': 'False', 'anger': 'False', 'sadness': 'False'}}]\n"
     ]
    }
   ],
   "source": [
    "prepended_data = prepend_labels_to_text(preprocessed_data, label_mapping)\n",
    "print(prepended_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'nikluge-2023-ea-dev-000001', 'prepended_text': '[CLS] 기대 하,,, [MASK] 내일 옥상 [MASK] [MASK] 하기 전에 표 구하길 기도해주세요 [SEP]', 'labels': {'joy': 'False', 'anticipation': 'True', 'trust': 'False', 'surprise': 'False', 'disgust': 'False', 'fear': 'False', 'anger': 'False', 'sadness': 'False'}}, {'id': 'nikluge-2023-ea-dev-000002', 'prepended_text': '[CLS] 기쁨 밴드 사운드 진짜 너무너무 좋다.... [MASK]인 [MASK] 진짜 [SEP]', 'labels': {'joy': 'True', 'anticipation': 'False', 'trust': 'False', 'surprise': 'False', 'disgust': 'False', 'fear': 'False', 'anger': 'False', 'sadness': 'False'}}, {'id': 'nikluge-2023-ea-dev-000003', 'prepended_text': '[CLS] 기쁨 칸 태리 너무 풋풋 [MASK] [MASK] 귀여워.. 근데 허리가 한줌 [MASK]셔 & [MASK]s & [SEP]', 'labels': {'joy': 'True', 'anticipation': 'False', 'trust': 'False', 'surprise': 'False', 'disgust': 'False', 'fear': 'False', 'anger': 'False', 'sadness': 'False'}}, {'id': 'nikluge-2023-ea-dev-000004', 'prepended_text': '[CLS] 기대 미스터초밥왕이 햄스터 [MASK]씨가 [MASK]어 오는 기분 [MASK]다는 더 [MASK]지 싶음.. [SEP]', 'labels': {'joy': 'False', 'anticipation': 'True', 'trust': 'False', 'surprise': 'False', 'disgust': 'False', 'fear': 'False', 'anger': 'False', 'sadness': 'False'}}, {'id': 'nikluge-2023-ea-dev-000005', 'prepended_text': '[CLS] 놀람 [UNK] 초연 자첫날 진심 한 마디 안에 멜로디 무시하고 가사 너무 [MASK] 들어간 [MASK] 같 [MASK]서 [MASK]?? [MASK] [MASK] 그 뒤로 그런극 많이 본 것 같아서 이제 놀랍진 [MASK]음 [SEP]', 'labels': {'joy': 'False', 'anticipation': 'False', 'trust': 'False', 'surprise': 'True', 'disgust': 'False', 'fear': 'False', 'anger': 'False', 'sadness': 'False'}}]\n"
     ]
    }
   ],
   "source": [
    "dev_prepended_data = prepend_labels_to_text(dev_preprocessed_data, label_mapping)\n",
    "print(dev_prepended_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, prepended_data, original_data, tokenizer):\n",
    "        self.prepended_data = prepended_data\n",
    "        self.original_data = original_data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prepended_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prepended_item = self.prepended_data[idx]\n",
    "        original_item = self.original_data[idx]\n",
    "        \n",
    "        prepended_text = prepended_item['prepended_text']\n",
    "        original_text = original_item['input']['form']\n",
    "\n",
    "        # 입력: prepend 데이터 / 출력: 원래 문장\n",
    "        prepended_encoding = self.tokenizer(prepended_text, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "        original_encoding = self.tokenizer(original_text, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "\n",
    "        # input IDs와 attention masks\n",
    "        prepended_input_ids = prepended_encoding['input_ids'].squeeze(0)\n",
    "        prepended_attention_mask = prepended_encoding['attention_mask'].squeeze(0)\n",
    "        original_input_ids = original_encoding['input_ids'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(prepended_input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(prepended_attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(original_input_ids, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmotionDataset(prepended_data, data, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = EmotionDataset(dev_prepended_data, dev_data, tokenizer)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=4e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/nlpgpu9/ellt/eojin/EA/증강/ELECTRA_DA\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2370 [00:00<?, ?it/s]/tmp/ipykernel_24313/3134132908.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(prepended_input_ids, dtype=torch.long),\n",
      "/tmp/ipykernel_24313/3134132908.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(prepended_attention_mask, dtype=torch.long),\n",
      "/tmp/ipykernel_24313/3134132908.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(original_input_ids, dtype=torch.long)\n",
      "Epoch 1:   0%|          | 0/2370 [09:18<?, ?it/s, loss=0.741]"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    num_train_batches = 0\n",
    "\n",
    "    train_loop = tqdm(train_dataloader, leave=True)\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        num_train_batches += 1\n",
    "\n",
    "        train_loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    \n",
    "    avg_train_loss = total_train_loss / num_train_batches\n",
    "    print(f\"Epoch {epoch+1}, Average Training Loss: {avg_train_loss}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    total_dev_loss = 0\n",
    "    num_dev_batches = 0\n",
    "\n",
    "    eval_loop = tqdm(dev_dataloader, leave=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dev_dataloader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_dev_loss += loss.item()\n",
    "            num_dev_batches += 1\n",
    "\n",
    "            eval_loop.set_description(f\"Epoch {epoch+1} Evaluation\")\n",
    "            eval_loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_dev_loss = total_dev_loss / num_dev_batches\n",
    "    print(f\"Epoch {epoch+1}, Average Dev Loss: {avg_dev_loss}\")\n",
    "\n",
    "    model_save_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}\")\n",
    "    model.save_pretrained(model_save_path)\n",
    "    print(f\"Saved model for epoch {epoch+1} at {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eojin_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
