import argparse
import json
import logging
import os
import sys

import torch
import numpy as np
from torch.optim import AdamW
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    EvalPrediction,
    AutoConfig,
    AutoModel,
    TrainerCallback,
    get_linear_schedule_with_warmup,
    get_cosine_schedule_with_warmup,
    get_cosine_with_hard_restarts_schedule_with_warmup
    )

from sklearn.model_selection import KFold
from datasets import Dataset
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, confusion_matrix, precision_score, recall_score, average_precision_score
from torch.optim.lr_scheduler import LambdaLR
from torch.optim.lr_scheduler import ReduceLROnPlateau
from run.LSTM_attention import *


parser = argparse.ArgumentParser(prog="train", description="Train Table to Text with BART")

g = parser.add_argument_group("Common Parameter")
g.add_argument("--output-dir", type=str, default="/home/nlpgpu9/ellt/eojin/EA/", help="output directory path to save artifacts")
g.add_argument("--model-path", type=str, default="Twitter/twhin-bert-large", help="model file path")
g.add_argument("--tokenizer", type=str, default="Twitter/twhin-bert-large", help="huggingface tokenizer path")
g.add_argument("--max-seq-len", type=int, default=218, help="max sequence length")
g.add_argument("--batch-size", type=int, default=32, help="training batch size")
g.add_argument("--valid-batch-size", type=int, default=64, help="validation batch size")
g.add_argument("--accumulate-grad-batches", type=int, default=1, help=" the number of gradident accumulation steps")
g.add_argument("--epochs", type=int, default=30, help="the numnber of training epochs")
g.add_argument("--learning-rate", type=float, default=1e-5, help="max learning rate")
g.add_argument("--weight-decay", type=float, default=5e-4, help="weight decay")
g.add_argument("--seed", type=int, default=42, help="random seed")
g.add_argument("--model-choice", type=str, default="AutoModelForSequenceClassification", help="or LSTM_attention or LSTM_multitask or loss_function")


def main(args):
    # device
    # device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger = logging.getLogger("train")
    logger.propagate = False
    logger.setLevel(logging.DEBUG)
    if not logger.handlers:
        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(logging.Formatter("[%(asctime)s] %(message)s"))
        logger.addHandler(handler)

    os.makedirs(args.output_dir, exist_ok=True)
    logger.info(f'[+] Save output to "{args.output_dir}"')

    logger.info(" ====== Arguements ======")
    for k, v in vars(args).items():
        logger.info(f"{k:25}: {v}")

    logger.info(f"[+] Set Random Seed to {args.seed}")
    np.random.seed(args.seed)
    os.environ["PYTHONHASHSEED"] = str(args.seed)
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed(args.seed)  # type: ignore

    logger.info(f'[+] Load Tokenizer"')
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)

    logger.info(f'[+] Load Dataset')
    train_ds = Dataset.from_json("/home/nlpgpu9/ellt/eojin/EA/동일감정제거_train+dev_수정.jsonl")
    # first_five = train_ds.select(range(5))
    valid_ds = Dataset.from_json("/home/nlpgpu9/ellt/eojin/EA/1434 (2).jsonl")
    test_ds = Dataset.from_json("/home/nlpgpu9/ellt/eojin/EA/nikluge-ea-2023-test_수정.jsonl")
    # first_five_t = test_ds.select(range(5))

    labels = list(train_ds["output"][0].keys())
    id2label = {idx:label for idx, label in enumerate(labels)}
    label2id = {label:idx for idx, label in enumerate(labels)}
    with open(os.path.join(args.output_dir, "label2id.json"), "w") as f:
        json.dump(label2id, f)

    def preprocess_data(examples):
        # take a batch of texts
        text1 = examples["input"]["form"]
        text2 = examples["input"]["target"]["form"]
        target_begin = examples["input"]["target"].get("begin")
        target_end = examples["input"]["target"].get("end")

        # encode them
        encoding = tokenizer(text1, text2, padding="max_length", truncation=True, max_length=args.max_seq_len)
        # add labels
        if examples["output"] != "":
            encoding["labels"] = [0.0] * len(labels)
            for key, idx in label2id.items():
                if examples["output"][key] == 'True':
                    encoding["labels"][idx] = 1.0


        # 타겟 찾기 (attention, multitask 위해)
        encoding["target_positions"] = [0] * len(encoding['input_ids'])  # 문장 길이만큼 0으로 초기화

        if text2 != None:
            encoded_target = tokenizer(text2, add_special_tokens=False)["input_ids"]
            encoded_text = tokenizer(text1, add_special_tokens=False)["input_ids"]

            for i in range(len(encoded_text) - len(encoded_target) + 1):
                if encoded_text[i:i+len(encoded_target)] == encoded_target:
                    target_begin = i + 1  # [CLS] 떄문에 + 1
                    target_end = i + len(encoded_target) + 1  # 나중에 리스트 슬라이싱 때문에 + 1
                    break

        # Mark the target positions with 1
            for i in range(target_begin, target_end):
                encoding["target_positions"][i] = 1  # 타겟이면 1, 타겟이 아니면 0

        return encoding


    encoded_tds = train_ds.map(preprocess_data, remove_columns=train_ds.column_names)
    encoded_vds = valid_ds.map(preprocess_data, remove_columns=valid_ds.column_names)
    encoded_test_ds = test_ds.map(preprocess_data, remove_columns=test_ds.column_names)

    logger.info(f'[+] Load Model from "{args.model_path}"')


    # config = AutoConfig.from_pretrained(args.model_path)
    # config.output_hidden_states = True
    # config.problem_type = "multi_label_classification"
    # config.num_labels = len(labels)
    # config.id2label = id2label
    # config.label2id = label2id
    # config.loss_fct_params = {"pos_weight": [0.24554828265765766, 0.5643400194111938, 2.3700747282608696, 1.9996656476881927, 3.449653922214898, 6.222502972651605, 3.240325077399381, 2.110131048387097]}

    class CustomTrainer(Trainer):
        def create_optimizer_and_scheduler(self, num_training_steps: int):
            self.optimizer = AdamW([
            {'params': self.model.model.parameters(), 'lr': args.learning_rate, 'weight_decay': args.weight_decay},
            {'params': self.model.bi_lstm.parameters(), 'lr': 1e-3, 'weight_decay': 5e-3},
            {'params': self.model.linear.parameters(), 'lr': 1e-3, 'weight_decay': 5e-3},
            {'params': self.model.gate_linear.parameters(), 'lr': 1e-3, 'weight_decay': 5e-3}
        ])

            self.lr_scheduler = get_linear_schedule_with_warmup(
                self.optimizer, 
                num_warmup_steps=self.args.warmup_steps,  # 1700
                num_training_steps=num_training_steps  # 17768
            )


    targs = TrainingArguments(
        output_dir=args.output_dir,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        learning_rate=args.learning_rate,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.valid_batch_size,
        num_train_epochs=args.epochs,
        weight_decay=args.weight_decay,
        load_best_model_at_end=True,
        metric_for_best_model= "f1",
        # optimizer_init=lambda _: custom_optimizer(model),
    )

    def multi_label_metrics(predictions, labels, threshold=0.5):
        # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)
        sigmoid = torch.nn.Sigmoid()
        probs = sigmoid(torch.Tensor(predictions))
        # next, use threshold to turn them into integer predictions
        y_pred = np.zeros(probs.shape)
        y_pred[np.where(probs >= threshold)] = 1
        # finally, compute metrics
        y_true = labels

        f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')
        roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')
        accuracy = accuracy_score(y_true, y_pred)

        tn, fp, fn, tp = confusion_matrix(y_true.ravel(), y_pred.ravel()).ravel()
        sensitivity = tp / (tp + fn)
        specificity = tn / (tn + fp)
        youden_j = sensitivity + specificity - 1

        precision = precision_score(y_true=y_true, y_pred=y_pred, average='micro')

        recall = recall_score(y_true=y_true, y_pred=y_pred, average='micro')


        # return as dictionary
        metrics = {'f1': f1_micro_average,
                   'sensitivity': sensitivity,
                   'specificity': specificity,
                   'roc_auc': roc_auc,
                   'accuracy': accuracy,
                   'youden_j': youden_j,
                   'precision': precision,
                   'recall': recall
                   }
        return metrics

    def compute_metrics(p: EvalPrediction):
        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
        result = multi_label_metrics(predictions=preds, labels=p.label_ids)
        with open(f"{args.output_dir}log.txt", "a") as f:
            f.write(json.dumps(result) + '\n')
        return result
        

    def jsonlload(fname):
        with open(fname, "r", encoding="utf-8") as f:
            lines = f.read().strip().split("\n")
            j_list = [json.loads(line) for line in lines]
        return j_list

    def jsonldump(j_list, fname):
        with open(fname, "w", encoding='utf-8') as f:
            for json_data in j_list:
                f.write(json.dumps(json_data, ensure_ascii=False)+'\n')


    model = AutoModelForSequenceClassification.from_pretrained(
        "/home/nlpgpu9/data/hufs/dhk/outputs/twhinbert_large__50_0.001/checkpoint-13639",
        problem_type="multi_label_classification",
        num_labels=len(labels),
        id2label=id2label,
        label2id=label2id
    )

    trainer = CustomTrainer(
        model,
        targs,
        train_dataset=encoded_tds,
        eval_dataset=encoded_vds,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[TestInferenceCallback()]
    )


    logger.info("[+] Eval mode & Disable gradient")
    model.eval()
    torch.set_grad_enabled(False)
    
    def run_inference(model, test_dataset, threshold_dict, compute_metrics, id2label):
        trainer = CustomTrainer(
            model,
            compute_metrics=compute_metrics
        )

        print("Starting inference...")
        predictions, label_ids, _ = trainer.predict(test_dataset)
        print("Prediction completed...")
        Softmax = torch.nn.Softmax(dim=-1)
        threshold_values = Softmax(torch.Tensor(predictions))

        outputs = []
        for thresholded in threshold_values:
            output = []
            for jdx, value in enumerate(thresholded):
                output.append(float(value) >= threshold_dict.get(jdx, 0.5))
            outputs.append(output)


            j_list = jsonlload("/home/nlpgpu9/ellt/eojin/EA/nikluge-ea-2023-test_수정.jsonl")

            for idx, oup in enumerate(outputs):
                j_list[idx]["output"] = {}

                # oup에서 True 또는 1인 값의 개수를 확인
                true_count = sum(oup)
                            
                if not any(oup):
                    max_index = threshold_values[idx].argmax().item()
                    oup[max_index] = True

                for jdx, v in enumerate(oup):
                    j_list[idx]["output"][id2label[jdx]] = ["True" if v else "False", threshold_values[idx][jdx].item()]
    
    
        jsonldump(j_list, "test_predictions_꼬링크.jsonl")


    threshold_dict = {
                0: 0.5, #joy
                1: 0.5, #anticipation
                2: 0.5, #trust
                3: 0.5, #surprise
                4: 0.5, #disgust
                5: 0.5, #fear
                6: 0.5, #anger
                7: 0.5 #sadness
        }
        
    run_inference(
        model,
        encoded_test_ds,
        threshold_dict,
        compute_metrics,
        id2label
    )
    

if __name__ == "__main__":
    exit(main(parser.parse_args()))
